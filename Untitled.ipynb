{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,time,re,copy,random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/home/xhsun/Desktop/MWP/multiencoderdecoder/MultiMath-master/data/Math_23K.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(filename):\n",
    "    data=[]\n",
    "    with open(filename,encoding='utf-8') as f:\n",
    "        lines=f.readlines()#lines是一个列表，每一个元素是文件中的一行，文件中的每7行组成一条训练样本\n",
    "        \n",
    "    json_string=''\n",
    "    #每7行是一个样本\n",
    "    for line_id,line_str in enumerate(lines):\n",
    "        json_string+=line_str\n",
    "        line_id+=1\n",
    "        if line_id%7==0:\n",
    "            example=json.loads(json_string)#json.loads可以将字典形式的字符串转换成一个字典\n",
    "            #example是一个字典,key值有'id','original_text','segmented_text','equation','ans'\n",
    "            if '千米/小时' in example['equation']:\n",
    "                example['equation']=example['equation'][:-5]#有些等式中含有（千米/小时）这个单位，把这个单位去掉\n",
    "            data.append(example)\n",
    "            json_string=''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_raw_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5001',\n",
       " 'original_text': '某电视机厂原来每天生产116台电视机，现在每天生产的台数是原来的12倍，现在每天能生产多少台电视机？',\n",
       " 'segmented_text': '某 电视机厂 原来 每天 生产 116 台 电视机 ， 现在 每天 生产 的 台数 是 原来 的 12 倍 ， 现在 每天 能 生产 多少 台 电视机 ？',\n",
       " 'equation': 'x=116*12',\n",
       " 'ans': '1392'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_num(data):\n",
    "    '''\n",
    "    将数据集中的每一个样本对应的文本问题中的数字替换成NUM\n",
    "    '''\n",
    "    #正则表达式中： +表示出现一次或多次，*表示出现零次或多次\n",
    "    #\\d*\\(\\d+/\\d+\\)\\d*  这个正则是为了匹配行如 (3/5)、2(3/5)、2(3/5)12 这类的数字（也就是带有括号的分数）\n",
    "    #\\d+\\.\\d+%? 这个正则是为了匹配行如 3.5 3.5% 这类的数字（也就是小数或者带有百分号的小数）\n",
    "    #\\d+%? 这个正则是为了匹配整数以及3%这类的带有百分号的整数\n",
    "    \n",
    "    pattern=re.compile(\"\\d*\\(\\d+/\\d+\\)\\d*|\\d+\\.\\d+%?|\\d+%?\")#pattern用来匹配问题文本中的所有数字\n",
    "    \n",
    "    pairs,generate_nums,generate_nums_dict=[],[],{}\n",
    "    copy_nums=0#copy_nums用来记录数据集中的所有问题中，哪一个问题中出现的数字次数最多，copy_nums用来记录这个次数\n",
    "    #copy_nums的数值影响着decoder端的词汇空间\n",
    "    \n",
    "    for example in data:\n",
    "        #example行如：{'id': '5001','original_text': '某电视机厂原来每天生产116台电视机，现在每天生产的台数是原来的12倍，现在每天能生产多少台电视机？',\n",
    "        #'segmented_text': '某 电视机厂 原来 每天 生产 116 台 电视机 ， 现在 每天 生产 的 台数 是 原来 的 12 倍 ， 现在 每天 能 生产 多少 台 电视机 ？',\n",
    "        #'equation': 'x=116*12','ans': '1392'}\n",
    "        idx=example['id']\n",
    "        nums=[]#nums用来记录根据pattern匹配出的问题文本中的所有数字\n",
    "        input_seq=[]#input_seq用来将问题文本中的所有数字替换成NUM\n",
    "        seg=example['segmented_text'].strip().split(' ')\n",
    "        #seg行如: ['某', '电视机厂', '原来', '每天', '生产', '116', '台', '电视机', '，', '现在', '每天', '生产', '的', '台数', '是', '原来', '的', '12', '倍', '，', '现在', '每天', '能', '生产', '多少', '台', '电视机', '？']\n",
    "        equation=example['equation'][2:]\n",
    "        #equations的形式行如： x=(25+14)/(1-(1/5)-(1/5));x=(11-1)*2;x=116*12等\n",
    "        for token in seg:\n",
    "            pos=re.search(pattern=pattern,string=token)#如果token是数字，那么pos返回的不是None\n",
    "            if pos and pos.start()==0:\n",
    "                nums.append(token[pos.start():pos.end()])\n",
    "                input_seq.append('NUM')#将所有数字替换成NUM\n",
    "                if pos.end()<len(token):\n",
    "                    #说明此时的token不仅仅含有数字，eg： 116千克，那么input_seq中要添加千克这个单词\n",
    "                    input_seq.append(token[pos.end():])\n",
    "            elif token!='':\n",
    "                #此时的token中没有数字\n",
    "                input_seq.append(token)\n",
    "        if copy_nums<len(nums):\n",
    "            copy_nums=len(nums)#copy_nums用来记录所有问题中出现数字次数最多的那个问题出现的数字的次数\n",
    "            \n",
    "        nums_fraction=[]#nums_fraction用来记录这个问题中出现的行如(2/5)这种带括号的分数数字\n",
    "        for num in nums:\n",
    "            if re.search('\\d*\\(\\d+/\\d+\\)\\d*',num):\n",
    "                nums_fraction.append(num)#num行如 5(2/5) (2/5) 5(2/5)5 这种，\n",
    "        nums_fraction=sorted(nums_fraction,key=lambda x:len(x),reverse=True)#将nums_fraction中的带括号的分数数字按照长度排序\n",
    "        #实验表明，排序或者不排序一点关系没有\n",
    "        def seg_and_tag(equation):\n",
    "            '''\n",
    "            seg_and_tag函数的作用是将equation，也就是表达式中的字符分割开，例如：equation='(25+14)/(1-(1/5)-(1/5))'\n",
    "            那么返回的表达式应该是['(', '25', '+', '14', ')', '/', '(', '1', '-', '(1/5)', '-', '(1/5)', ')']\n",
    "            同时要将各个数字替换成Ni，i代表这个数字在问题文本中出现的顺序\n",
    "            这也是为什么前面要用nums_fraction专门保存带括号的分数，这样才能使得整个括号和分数看成一个整体\n",
    "            '''\n",
    "            res=[]\n",
    "            for num in nums_fraction:\n",
    "                #如果nums_fraction是空列表,也就是说当前问题没有带括号的分数,那么这个for循环自然不会执行\n",
    "                if num in equation:\n",
    "                    #从equation中找到这个带括号的分数的位置\n",
    "                    p_start=equation.find(num)\n",
    "                    p_end=p_start+len(num)\n",
    "                    if p_start>0:\n",
    "                        #以上面的equation为例子，显然此时num等于(1/5)，所以p_start>0，此时我们需要处理(25+14)/(1-\n",
    "                        res+=seg_and_tag(equation[:p_start])\n",
    "                    if nums.count(num)==1:\n",
    "                        #也就是说这个数字仅在问题文本中出现过一次，那么此时就可以用Ni代替这个数字，\n",
    "                        #i表示的是这个数字在文本中出现的顺序\n",
    "                        res.append('N'+str(nums.index(num)))\n",
    "                    else:\n",
    "                        #说明这个数字在问题文本中出现了多次，那么此时直接记录这个数字，而不用Ni替代\n",
    "                        res.append(num)\n",
    "                    if p_end<len(equation):\n",
    "                        res+=seg_and_tag(equation[p_end:])#递归右边的部分\n",
    "                    return res\n",
    "            #现在已经将这类括号带分数的数字处理完毕，接下来处理整数、小数、百分数\n",
    "            number_position=re.search(pattern='\\d+\\.\\d+%?|\\d+%?',string=equation)\n",
    "            if number_position:\n",
    "                p_start=number_position.start()\n",
    "                p_end=number_position.end()\n",
    "                if p_start>0:\n",
    "                    #类似的，递归左边\n",
    "                    res+=seg_and_tag(equation[:p_start])\n",
    "                number=equation[p_start:p_end]\n",
    "                if nums.count(number)==1:\n",
    "                    res.append('N'+str(nums.index(number)))\n",
    "                else:\n",
    "                    res.append(number)\n",
    "                if p_end<len(equation):\n",
    "                    res+=seg_and_tag(equation[p_end:])\n",
    "                return res\n",
    "            #上面的代码是用来处理数字的，如：带有括号的分数、小数、整数、百分数等\n",
    "            #下面的for循环处理equation中的 括号和+-/*\n",
    "            for rest_op in equation:\n",
    "                #rest_op要么是括号()，要么是+-/*\n",
    "                res.append(rest_op)\n",
    "            return res\n",
    "        \n",
    "        output_seq=seg_and_tag(equation=equation)#output_seq就是decoder端要生成的表达式标签\n",
    "        \n",
    "        for token in output_seq:\n",
    "            if token[0].isdigit() and token not in generate_nums and token not in nums:\n",
    "                #说明此时这是一个数字，并且这个数字没有出现在问题中，这类数字包括1或者3.14这种常数\n",
    "                generate_nums.append(token)\n",
    "                generate_nums_dict[token]=1\n",
    "            if token in generate_nums and token not in nums:\n",
    "                generate_nums_dict[token]+=1\n",
    "        \n",
    "        num_pos=[]#num_pos用来记录每一个数字的位置将equation\n",
    "        for i,j in enumerate(input_seq):\n",
    "            #input_seq是将问题中的所有数字替换成NUM后的变量\n",
    "            if j=='NUM':\n",
    "                num_pos.append(i)\n",
    "        assert len(nums)==len(num_pos)\n",
    "        #nums记录的是每一个数字，num_pos记录的是每一个数字的位置\n",
    "        pairs.append((idx,input_seq,output_seq,nums,num_pos))\n",
    "        \n",
    "    #结束for循环后，我们就已经处理了所有的问题，接下来统计数据集中频繁出现的常数\n",
    "    temp_g=[]#用来记录数据集中频繁出现的常数，比如3.14\n",
    "    for g in generate_nums:\n",
    "        if generate_nums_dict[g]>=5:\n",
    "            temp_g.append(g)\n",
    "    return pairs,temp_g,copy_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs,generate_nums,copy_nums=transfer_num(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集中频繁出现的常数 :  ['1', '3.14']\n",
      "数据集的所有问题中，出现数字次数最多那个问题中，数字出现的次数 :  15 这个数字的大小决定decoder端最少要生成多少个单词\n"
     ]
    }
   ],
   "source": [
    "print(\"数据集中频繁出现的常数 : \",generate_nums)\n",
    "print(\"数据集的所有问题中，出现数字次数最多那个问题中，数字出现的次数 : \",copy_nums,\\\n",
    "      \"这个数字的大小决定decoder端最少要生成多少个单词\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '15541',\n",
       " 'original_text': '农场有羊500头，牛比羊的头数多(1/4)，牛有多少头？',\n",
       " 'segmented_text': '农场 有 羊 500 头 ， 牛 比 羊 的 头数 多 (1/4) ， 牛 有 多少 头 ？',\n",
       " 'equation': 'x=500*(1+(1/4))',\n",
       " 'ans': '625'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[15540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('15541', ['农场', '有', '羊', 'NUM', '头', '，', '牛', '比', '羊', '的', '头数', '多', 'NUM', '，', '牛', '有', '多少', '头', '？'], ['N0', '*', '(', '1', '+', 'N1', ')'], ['500', '(1/4)'], [3, 12])\n"
     ]
    }
   ],
   "source": [
    "print(pairs[15540])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(\"\\d*\\(\\d+/\\d+\\)\\d*|\\d+\\.\\d+%?|\\d+%?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=re.search(pattern=pattern,string='116千米')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_trained=json.load(open(\"/home/xhsun/Desktop/MWP/multiencoderdecoder/MultiMath-master/data/train_4.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18528"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15541', ['农场', '有', '羊', 'NUM', '头', '，', '牛', '比', '羊', '的', '头数', '多', 'NUM', '，', '牛', '有', '多少', '头', '？'], ['n', 'v', 'n', 'ws', 'n', 'wp', 'n', 'p', 'n', 'u', 'n', 'a', 'ws', 'wp', 'n', 'v', 'r', 'q', 'wp'], [1, -1, 4, 4, 1, 1, 12, 12, 7, 8, 7, 12, 15, 12, 15, 1, 17, 15, 1], ['*', 'N0', '+', '1', 'N1'], ['N0', '1', 'N1', '+', '*'], ['500', '(1/4)'], [3, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(pairs_trained[15540])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_infix_to_prefix(expression):\n",
    "    operator_stack=[]#运算符栈\n",
    "    operand_stack=[]#操作数栈\n",
    "    operator_priority={'+':0,'-':0,'*':1,'/':1,'^':2}\n",
    "    expression=deepcopy(expression)#deepcopy是深拷贝\n",
    "    expression.reverse()#转前缀的过程是从右至左扫描\n",
    "    for e in expression:\n",
    "        if e in [')',']']:\n",
    "            #当遇到右括号时，直接进栈\n",
    "            operator_stack.append(e)\n",
    "        elif e =='(':\n",
    "            #弹出栈中的运算符，直到遇到)为止\n",
    "            temp=operator_stack.pop()\n",
    "            while temp!=')':\n",
    "                operand_stack.append(temp)\n",
    "                temp=operator_stack.pop()\n",
    "        elif e=='[':\n",
    "            #弹出栈中的运算符，直到遇到]为止\n",
    "            temp=operator_stack.pop()\n",
    "            while temp!=']':\n",
    "                operand_stack.append(temp)\n",
    "                temp=operator_stack.pop()\n",
    "        elif e in operator_priority:\n",
    "            #此时是运算符，需要比较优先级，当栈顶运算符的优先级大于e的优先级时，就一直弹栈\n",
    "            #不过需要注意的是，如果栈顶是右括号，那么就不能再弹了，因为右括号要等到左括号来了才能弹栈\n",
    "            while len(operator_stack)>0 and operator_stack[-1] not in [')',']'] and operator_priority[e]<operator_priority[operator_stack[-1]]:\n",
    "                operand_stack.append(operator_stack.pop())\n",
    "            operator_stack.append(e)\n",
    "        else:\n",
    "            #说明此时的e是操作数\n",
    "            operand_stack.append(e)\n",
    "    #将运算符栈中的剩余运算符全部弹出到操作数栈中\n",
    "    while len(operator_stack)>0:\n",
    "        operand_stack.append(operator_stack.pop())\n",
    "    operand_stack.reverse()\n",
    "    return operand_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', 'a', '*', 'b', '-', 'c', 'd']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression=list('a+b*(c-d)')\n",
    "from_infix_to_prefix(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_infix_to_postfix(expression):\n",
    "    operator_stack=[]\n",
    "    operand_stack=[]\n",
    "    expression=deepcopy(expression)\n",
    "    operator_priority={'+':0,'-':0,'*':1,'/':1,'^':2}\n",
    "    for e in expression:\n",
    "        if e in ['(','[']:\n",
    "            operator_stack.append(e)\n",
    "        elif e ==')':\n",
    "            temp=operator_stack.pop()\n",
    "            while temp!='(':\n",
    "                operand_stack.append(temp)\n",
    "                temp=operator_stack.pop()\n",
    "        elif e ==']':\n",
    "            temp=operator_stack.pop()\n",
    "            while temp!='[':\n",
    "                operand_stack.append(temp)\n",
    "                temp=operator_stack.pop()\n",
    "        elif e in operator_priority:\n",
    "            while len(operator_stack)>0 and operator_stack[-1] not in ['(','['] and operator_priority[e]<operator_priority[operator_stack[-1]]:\n",
    "                operand_stack.append(operator_stack.pop())\n",
    "            operator_stack.append(e)\n",
    "        else:\n",
    "            operand_stack.append(e)\n",
    "    while len(operator_stack)>0:\n",
    "        operand_stack.append(operator_stack.pop())\n",
    "    return operand_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', '-', '*', '+']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression=list('a+b*(c-d)')\n",
    "from_infix_to_postfix(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '501',\n",
       " 'original_text': '新世纪百货开展“庆6一”促销活动，一种童鞋的价格下降20%后再降8元，这时的价格是56元．这种童鞋的原价=多少元？',\n",
       " 'segmented_text': '新世纪 百货 开展 “ 庆 6 一 ” 促销 活动 ， 一种 童鞋 的 价格下降 20% 后 再 降 8 元 ， 这时 的 价格 是 56 元 ． 这种 童鞋 的 原价 = 多少 元 ？',\n",
       " 'equation': 'x=(56+8)/(1-20%)',\n",
       " 'ans': '80'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('501', ['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', '童鞋', '的', '价格下降', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', '童鞋', '的', '原价', '=', '多少', '元', '？'], ['(', 'N3', '+', 'N2', ')', '/', '(', '1', '-', 'N1', ')'], ['6', '20%', '8', '56'], [5, 15, 19, 26])\n"
     ]
    }
   ],
   "source": [
    "print(pairs[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(math23k_file):\n",
    "    data=load_raw_data(math23k_file)#data的每一个元素是一个dict，字段有：id,original_text,segmented_text,equation,ans\n",
    "    pairs,generate_nums,copy_nums=transfer_num(data)\n",
    "    #pairs是将data的每一个数据里面的segmented_text中的数字转换成NUM，将equation中的数字转换成Ni，其中i\n",
    "    #代表这个数字在问题中出现的顺序，pairs还有两个元素，分别记录问题对应的所有数字和数字的位置\n",
    "    \n",
    "    pre_temp_pairs=[]\n",
    "    for p in pairs:\n",
    "        #p[0]是id,p[1]是行如['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '， '再', '降', 'NUM', '？'],\n",
    "        #这样的问题\n",
    "        postags=postagger.post(p[1])#也就是标注问题中的每一个单词的词性\n",
    "        arcs=parser.parse(p[1],postags)#提取整个句子的句法\n",
    "        parse_tree=[arc[0]-1 for arc in arcs]#其中arc是一个元祖(id,relation)，id代表的就是当前这个单词与哪一个单词有关联，\n",
    "        #id表示的就是那个单词在整个句子中的索引，但是由于ROOT这个单词默认占据0，所以单词的实际位置需要-1\n",
    "        #relation表示的就是句法关系\n",
    "        \n",
    "        pre_temp_pairs.append((p[0],p[1],postags,parse_tree,\n",
    "                               from_infix_to_prefix(p[2]),from_infix_to_postfix(p[2]),p[3],p[4]))\n",
    "        #其中p[3]和p[4]分别是nums和nums_pos，也就是这个问题中所有的数字和数字的位置\n",
    "        #p[2]就是中缀表达式，现在已经转换成前缀和后缀了\n",
    "    pairs=pre_temp_pairs\n",
    "    #接下来构造5折交叉验证的数据集\n",
    "    fold_size=int(len(pairs)*0.2)#fold_size也就是每一折的测试集合大小，在math23k上约等于4632\n",
    "    fold_pairs=[]\n",
    "    for split_fold in range(4):\n",
    "        fold_start=fold_size*split_fold\n",
    "        fold_end=fold_size*(split_fold+1)\n",
    "        fold_pairs.append(pairs[fold_start:fold_end])\n",
    "    #split_fold==0,1,2,3\n",
    "    #fold_pairs==[pairs[0:4632],pairs[4632:9264],pairs[9264:13896],pairs[13896:18528]]\n",
    "    fold_pairs.append(pairs[fold_size*4:])#fold_pairs==[pairs[0:4632],pairs[4632:9264],pairs[9264:13896],pairs[13896:18528],pairs[18528:23162]]\n",
    "    \n",
    "    for fold in range(5):\n",
    "        pairs_tested=[]\n",
    "        pairs_trained=[]\n",
    "        for fold_t in range(5):\n",
    "            if fold_t==fold:\n",
    "                #当fold==0时，就用fold_pairs[0]作为测试集，其它四个作为训练集\n",
    "                pairs_tested+=fold_pairs[fold_t]\n",
    "            else:\n",
    "                pairs_trained+=fold_pairs[fold_t]\n",
    "        with open(\"data/train\"+str(fold)+\".json\",'w') as f:\n",
    "            json.dump(pairs_trained,f,ensure_ascii=False,indent=4)\n",
    "        with open(\"data/test\"+str(fold)+\".json\",\"w\") as f:\n",
    "            json.dump(pairs_tested,f,ensure_ascii=False,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_trained=json.load(open(\"/home/xhsun/Desktop/MWP/multiencoderdecoder/MultiMath-master/data/train_4.json\"))\n",
    "pairs_tested=json.load(open(\"/home/xhsun/Desktop/MWP/multiencoderdecoder/MultiMath-master/data/test_4.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18528 4634\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs_trained),len(pairs_tested))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example id :  11\n",
      "example input seq :  ['要', '修', '一段', '长', 'NUM', '千米', '的', '路', '，', '第一天', '修', '了', 'NUM', '千米', '，', '第', '二', '天', '修', '了', '余下', '的', 'NUM', '，', '还', '剩下', '多少', '千米', '没有', '修', '完', '？']\n",
      "example question pos(pos指的是词性) :  ['v', 'v', 'm', 'a', 'ws', 'q', 'u', 'n', 'wp', 'nt', 'v', 'u', 'ws', 'q', 'wp', 'm', 'm', 'q', 'v', 'u', 'v', 'u', 'ws', 'wp', 'd', 'v', 'r', 'q', 'd', 'v', 'v', 'wp']\n",
      "example syntatic parser(句法分析) :  [1, -1, 5, 5, 5, 7, 5, 1, 1, 10, 1, 10, 13, 10, 10, 16, 17, 18, 1, 18, 22, 20, 18, 18, 25, 18, 27, 25, 29, 25, 29, 1]\n",
      "example prefix expression :  ['-', '-', 'N0', '*', '-', 'N0', 'N1', 'N2', 'N1']\n",
      "example postfix expression :  ['N0', 'N0', 'N1', '-', 'N2', '*', '-', 'N1', '-']\n",
      "example question nums :  ['24', '4', '(3/8)']\n",
      "example question nums_pos :  [4, 12, 22]\n"
     ]
    }
   ],
   "source": [
    "train_example=pairs_trained[10]\n",
    "print(\"example id : \",train_example[0])\n",
    "print(\"example input seq : \",train_example[1])\n",
    "print(\"example question pos(pos指的是词性) : \",train_example[2])\n",
    "print(\"example syntatic parser(句法分析) : \",train_example[3])\n",
    "print(\"example prefix expression : \",train_example[4])\n",
    "print(\"example postfix expression : \",train_example[5])\n",
    "print(\"example question nums : \",train_example[6])\n",
    "print(\"example question nums_pos : \",train_example[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token=0#默认pad位置用0填充\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index={}#词到id的转换字典\n",
    "        self.word2count={}#词到词频的转换字典\n",
    "        self.index2word=[]\n",
    "        self.n_words=0\n",
    "        self.num_start=0\n",
    "    \n",
    "    def add_sen_to_vocab(self,sentence):\n",
    "        #传进来的sentence要么是问题文本，行如:['要', '修', '一段', '长', 'NUM', '千米', '的', '路', '，', '第一天', '修', '了', 'NUM', '千米', '，', '第', '二', '天', '修', '了', '余下', '的', 'NUM', '，', '还', '剩下', '多少', '千米', '没有', '修', '完', '？']\n",
    "        #要么是句子的标注词性，行如['v', 'v', 'm', 'a', 'ws', 'q', 'u', 'n', 'wp', 'nt', 'v', 'u', 'ws', 'q', 'wp', 'm', 'm', 'q', 'v', 'u', 'v', 'u', 'ws', 'wp', 'd', 'v', 'r', 'q', 'd', 'v', 'v', 'wp']\n",
    "        #这是因为论文有两个encoder，之前的论文只有一个encoder，只需要问题文本作为输入\n",
    "        for word in sentence:\n",
    "            if re.search(pattern='N\\d+|NUM|\\d+',string=word):\n",
    "                continue#数字和特殊字符NUM不作为encoder端的词汇\n",
    "            if word not in self.index2word:\n",
    "                self.word2index[word]=self.n_words\n",
    "                self.word2count[word]=1\n",
    "                self.index2word.append(word)\n",
    "                self.n_words+=1\n",
    "            else:\n",
    "                self.word2count[word]+=1\n",
    "    def trim(self,min_count):\n",
    "        '''\n",
    "        根据min_count去除词典中的单词，缩小词典的空间\n",
    "        '''\n",
    "        keep_words=[]\n",
    "        for word,freq in self.word2count.items():\n",
    "            if freq>=min_count:\n",
    "                #词频高的词保留\n",
    "                keep_words.append(word)\n",
    "        self.word2index={}\n",
    "        self.word2count={}\n",
    "        self.index2word=[]\n",
    "        self.n_words=0\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.word2index[word]=self.n_words\n",
    "            self.index2word.append(word)\n",
    "            self.n_words+=1\n",
    "    \n",
    "    def build_input_lang(self,trim_min_count):\n",
    "        if trim_min_count>0:\n",
    "            self.trim(min_count=trim_min_count)\n",
    "            self.index2word=['PAD','NUM','UNK']+self.index2word#因为删除了一些单词后，在训练集中自然会出现一些没有见过的单词\n",
    "        else:\n",
    "            self.index2word=['PAD','NUM']+self.index2word\n",
    "        #重置word2index，因为要考虑PAD和NUM以及UNK等特殊字符\n",
    "        self.word2index={word:index for index,word in enumerate(self.index2word)}\n",
    "    \n",
    "    def build_input_lang_for_pos(self):\n",
    "        #对于词性标注的输入，没有NUM需要考虑,而且不需要删除不常见单词\n",
    "        self.index2word=['PAD','UNK']+self.index2word#需要注意的是，调用这个函数的对象一定是词性标注输入的对象\n",
    "        self.n_words=len(self.index2word)\n",
    "        self.word2index={word:index for index,word in enumerate(self.index2word)}\n",
    "    \n",
    "    def build_output_lang(self,generate_nums,copy_nums):\n",
    "        '''\n",
    "        generate_nums代表的是常数，如: 1,3.14\n",
    "        copy_nums代表的是出现数字次数最多的那个问题出现的数字次数，copy_nums决定了decoder端最多可以预测多少个不同数字\n",
    "        '''\n",
    "        self.index2word+=['PAD','EOS']+generate_nums+['N'+str(i) for i in range(copy_nums)]+['SOS','UNK']\n",
    "        self.n_words=len(self.index2word)\n",
    "        self.word2index={word:index for index,word in enumerate(self.index2word)}\n",
    "    def build_output_lang_for_tree(self,generate_nums,copy_nums):\n",
    "        '''\n",
    "        树形结构的decoder和sequence结构的decoder是不同的,因为tree结构不是序列式的生成表达式,所以不考虑PAD和EOS,SOS等\n",
    "        '''\n",
    "        self.num_start=len(self.index2word)\n",
    "        self.index2word+=generate_nums+['N'+str(i) for i in range(copy_nums)]+['UNK']\n",
    "        self.n_words=len(self.index2word)\n",
    "        self.word2index={word:index for index,word in enumerate(self.index2word)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_lang = Lang()\n",
    "input2_lang = Lang()\n",
    "output1_lang = Lang()\n",
    "output2_lang = Lang()\n",
    "\n",
    "for pair in pairs_trained:\n",
    "    if pair[-1]:\n",
    "        input1_lang.add_sen_to_vocab(pair[1])#pair[1]是问题文本\n",
    "        input2_lang.add_sen_to_vocab(pair[2])#pair[2]是问题句子的词性\n",
    "        output1_lang.add_sen_to_vocab(pair[4])#pair[4]是前缀表达式\n",
    "        output2_lang.add_sen_to_vocab(pair[5])#pair[5]是后缀表达式\n",
    "        \n",
    "trim_min_count=5\n",
    "input1_lang.build_input_lang(trim_min_count)\n",
    "input2_lang.build_input_lang_for_pos()\n",
    "output1_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "output2_lang.build_output_lang(generate_nums, copy_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': 0, 'NUM': 1, 'UNK': 2, '雅乐': 3, '学校': 4, '二年级': 5, '的': 6, '小朋友': 7, '到': 8, '一条': 9, '小路': 10, '一边': 11, '植树': 12, '．': 13, '们': 14, '每隔': 15, '米': 16, '种': 17, '一棵树': 18, '（': 19, '马路': 20, '两头': 21, '都': 22, '了': 23, '树': 24, '）': 25, '，': 26, '最后': 27, '发现': 28, '一共': 29, '棵': 30, '这': 31, '条': 32, '长': 33, '多少': 34, '一': 35, '个': 36, '工程队': 37, '挖土': 38, '第一天': 39, '挖': 40, '方': 41, '从': 42, '第': 43, '二': 44, '天': 45, '开始': 46, '每天': 47, '连续': 48, '这个': 49, '一周': 50, '共': 51, '？': 52, '小': 53, '明': 54, '看': 55, '一本': 56, '故事书': 57, '全书': 58, '页': 59, '页数': 60, '是': 61, '前两天': 62, '总数': 63, '这时': 64, '还有': 65, '没有': 66, '那么': 67, '本书': 68, '甲地': 69, '乙地': 70, '如果': 71, '骑': 72, '自行车': 73, '每': 74, '小时': 75, '行驶': 76, '千米': 77, '可以': 78, '到达': 79, '乘': 80, '汽车': 81, '只': 82, '需要': 83, '刚': 84, '体重': 85, '千克': 86, '小强': 87, '倍': 88, '=': 89, '-': 90, '与': 91, '积': 92, '所得': 93, '差': 94, '除以': 95, '商': 96, '原有': 97, '学生': 98, '人': 99, '月份': 100, '毕业': 101, '招': 102, '入': 103, '新生': 104, '比': 105, '原来': 106, '增加': 107, '百分之几': 108, '买': 109, '一套': 110, '住房': 111, '售价': 112, '万元': 113, '每年': 114, '付': 115, '年': 116, '后': 117, '才能': 118, '王': 119, '叔叔': 120, '开车': 121, '去': 122, '以': 123, '/': 124, '时': 125, '速度': 126, '行': 127, '才': 128, '返回': 129, '由于': 130, '有': 131, '任务': 132, '加快': 133, '只用': 134, '就': 135, '回到': 136, '果园': 137, '李': 138, '数': 139, '桃树': 140, '梨树': 141, '已知': 142, '要': 143, '修': 144, '一段': 145, '路': 146, '余下': 147, '还': 148, '剩下': 149, '完': 150, '公路': 151, '原': 152, '计划': 153, '完成': 154, '实际': 155, '多': 156, '这样': 157, '甲乙': 158, '两列': 159, '火车': 160, '同时': 161, '相距': 162, '两地': 163, '相对': 164, '开出': 165, '甲': 166, '车': 167, '两车': 168, '乙': 169, '*': 170, '已经': 171, '没': 172, '红': 173, '跳绳': 174, '下': 175, '军': 176, '跳': 177, '每秒': 178, '能': 179, '飞行': 180, '卫星': 181, '少': 182, '共有': 183, '芳': 184, '家': 185, '吨': 186, '每吨': 187, '水': 188, '价格': 189, '元': 190, '口': 191, '平均': 192, '每人': 193, '应交': 194, '水费': 195, '个数': 196, '余': 197, '班': 198, '男生': 199, '女生': 200, '体育课': 201, '上': 202, '周': 203, '老师': 204, '把': 205, '他们': 206, '站': 207, '一队': 208, '队': 209, '列': 210, '综合': 211, '算式': 212, '在': 213, '一次': 214, '数学': 215, '竞赛': 216, '中共': 217, '道': 218, '题': 219, '做': 220, '对': 221, '得': 222, '分': 223, '错': 224, '或': 225, '不': 226, '扣': 227, '华': 228, '他': 229, '几道': 230, '购买': 231, '每张': 232, '单价': 233, '课桌': 234, '张': 235, '用': 236, '这笔': 237, '钱': 238, '椅子': 239, '一个': 240, '数多': 241, '商店': 242, '里': 243, '梨': 244, '苹果': 245, '组成': 246, '辆': 247, '压路机': 248, '每分钟': 249, '压路': 250, '宽度': 251, '为': 252, '分钟': 253, '平方米': 254, '早晨': 255, '教室': 256, '名': 257, '其中': 258, '占': 259, '总': 260, '人数': 261, '后来': 262, '又': 263, '来': 264, '几名': 265, '存入': 266, '银行': 267, '定期': 268, '整存整取': 269, '按': 270, '年利率': 271, '计算': 272, '到期': 273, '可': 274, '利息': 275, '东': 276, '拿': 277, '文具': 278, '铅笔': 279, '已': 280, '本': 281, '蝴蝶': 282, '一只': 283, '蜜蜂': 284, '飞行速度': 285, '开展': 286, '课外活动': 287, '全班': 288, '参加': 289, '科技': 290, '组': 291, '阅读': 292, '两个': 293, '几分': 294, '之': 295, '几': 296, '、': 297, 'A': 298, 'B': 299, '两城': 300, '出发': 301, '相向': 302, '而': 303, '经过': 304, '一段时间': 305, '车行': 306, '全程': 307, '大伯': 308, '卖出': 309, '两桶': 310, '相同': 311, '一桶': 312, '另': 313, '；': 314, '所': 315, '卖': 316, '相差': 317, '每千克': 318, '价钱': 319, '正方形': 320, '栽': 321, '柳树': 322, '两棵': 323, '之间': 324, '间隔': 325, '周长': 326, '件': 327, '工作': 328, '现在': 329, '先': 330, '由': 331, '继续': 332, '几天': 333, '全部': 334, '人民': 335, '取回': 336, '某地': 337, '日照时间': 338, '它': 339, '短': 340, '药水': 341, '药': 342, '和': 343, '：': 344, '配制': 345, '成': 346, '这种': 347, '水果店': 348, '运': 349, '箱': 350, '生': 351, '桔子': 352, '某': 353, '每箱': 354, '篮球': 355, '数量': 356, '足球': 357, '(': 358, ')': 359, '修路': 360, '修好': 361, '全长': 362, '仓': 363, '粮食': 364, '若干': 365, '运出': 366, '运走': 367, '存有': 368, '台': 369, '电视机': 370, '原价': 371, '提价': 372, '降价': 373, '一台': 374, '妈妈': 375, '蓝': 376, '兰': 377, '几倍': 378, '特快': 379, '列车': 380, '一批': 381, '水果': 382, '第一次': 383, '次': 384, '两次': 385, '正好': 386, '批': 387, '前': 388, '追': 389, '刘': 390, '一样': 391, '高': 392, '厘米': 393, '+': 394, '超市': 395, '会员卡': 396, '购物': 397, '打': 398, '王老师': 399, '准备': 400, '联欢会': 401, '品牌': 402, '饮料': 403, '使用': 404, '便宜': 405, '余数': 406, '共同': 407, '开凿': 408, '隧道': 409, '甲队': 410, '快': 411, '车长': 412, '慢': 413, '同': 414, '向': 415, '当': 416, '车头': 417, '快车': 418, '几秒': 419, '慢车': 420, '同学': 421, '节日': 422, '送': 423, '鲜花': 424, '给': 425, '其余': 426, '贺卡': 427, '店里': 428, '吃': 429, '剩': 430, '根': 431, '绳子': 432, '剪': 433, '一根': 434, '最': 435, '几根': 436, '爬': 437, '行走': 438, '完全': 439, '不同': 440, '地': 441, '向前': 442, '中': 443, '山脚': 444, '山顶': 445, '走': 446, '约': 447, '路程': 448, '早上': 449, '跑步': 450, '大约': 451, '途中': 452, '相遇': 453, '走廊': 454, '柱子': 455, '底面': 456, '油漆': 457, '漆': 458, '被减数': 459, '减数': 460, '书': 461, '工人': 462, '木料': 463, '锯': 464, '段': 465, '保持': 466, '不变': 467, '再': 468, '两段': 469, '大爷': 470, '年期': 471, '国家': 472, '建设': 473, '债券': 474, '获': 475, '本金': 476, '出租车': 477, '起步价': 478, '内': 479, '超过': 480, '多付': 481, '星期天': 482, '坐': 483, '爷爷奶奶': 484, '爸爸': 485, '请': 486, '你': 487, '算': 488, '远': 489, '六年级': 490, '货车': 491, '客车': 492, '平行四边形': 493, '底边': 494, '面积': 495, '平方分米': 496, '皮球': 497, '校园': 498, '杨树': 499, '松树': 500, '小学': 501, '买来': 502, '排球': 503, '各': 504, '共用': 505, '块': 506, '公顷': 507, '菜地': 508, '需': 509, '施肥': 510, '童话': 511, '读完': 512, '照': 513, '新光': 514, '一种': 515, '出售': 516, '运输队': 517, '运输': 518, '套': 519, '玻璃': 520, '茶具': 521, '每套': 522, '运费': 523, '损坏': 524, '不仅': 525, '不得': 526, '还要': 527, '费': 528, '扣除': 529, '结果': 530, '得到': 531, '过程': 532, '距离': 533, '她': 534, '离家': 535, '大': 536, '运进': 537, '橘子': 538, '重量': 539, '桶': 540, '油': 541, '倒': 542, '出': 543, '丁丁': 544, '带': 545, '现价': 546, '身高': 547, 'cm': 548, '父亲': 549, '浓度': 550, '克': 551, '糖水': 552, '加多少': 553, '两辆': 554, '几小时': 555, '款': 556, '空调': 557, '先后': 558, '公园': 559, '一圈': 560, '公里': 561, '比赛': 562, '跑': 563, '圈': 564, '电视机厂': 565, '今年': 566, '生产': 567, '去年': 568, '产量比': 569, '某班': 570, '存放': 571, '两': 572, '南京': 573, '铁路': 574, '十': 575, '半月': 576, '下半月': 577, '超额完成': 578, '月': 579, '鹅': 580, '孵化期': 581, '等于': 582, '鸭': 583, '鸡': 584, '公共汽车': 585, '开': 586, '下车': 587, '但': 588, '上来': 589, '上车': 590, '着': 591, '五年级': 592, '一排': 593, '至少': 594, 'AB': 595, '程': 596, '儿童': 597, '上衣': 598, '一件': 599, '成人': 600, '多少倍': 601, '…': 602, '工厂': 603, '活动': 604, '第一': 605, '车间': 606, '光明': 607, '学期': 608, '书画': 609, '兴趣小组': 610, '本学期': 611, '相等': 612, '则': 613, '求': 614, '长方体': 615, '盒子': 616, '宽': 617, '盛': 618, '圆柱': 619, '体积': 620, '立方厘米': 621, '底': 622, '平方厘米': 623, '文化': 624, '广场': 625, '树苗': 626, '每行': 627, '株': 628, '美术': 629, '小组': 630, '音乐': 631, '既': 632, '也': 633, '降低': 634, '组织': 635, '四年级': 636, '开往': 637, '金': 638, '湖': 639, '平方千米': 640, '陆地': 641, '农具厂': 642, '加工': 643, '零件': 644, '提前': 645, '头': 646, '奶牛': 647, '草': 648, '机器': 649, 'h': 650, '为了': 651, '食堂': 652, '每桶': 653, '质量': 654, '取出': 655, '重': 656, '未': 657, '塑料': 658, '装': 659, '箱子': 660, '货物': 661, '要求': 662, '每次': 663, '相当于': 664, '厂': 665, '女职工': 666, '全厂': 667, '职工': 668, '办公桌': 669, '桌椅': 670, '一张': 671, '桌子': 672, '一把': 673, '一侧': 674, '从头到尾': 675, '立': 676, '电线杆': 677, '刚好': 678, '知道': 679, '两天': 680, '多长': 681, '男女生': 682, '苹果树': 683, '阿姨': 684, '一份': 685, '稿件': 686, '一些': 687, '此时': 688, '这份': 689, '实验': 690, '“': 691, '环保': 692, '”': 693, '回收': 694, '废纸': 695, '据': 696, '率': 697, '这些': 698, '再生纸': 699, '西服': 700, '裤子': 701, '盐': 702, '溶解': 703, '里面': 704, '盐水': 705, '赵老师': 706, '枝': 707, '同样': 708, '钢笔': 709, '想': 710, '上午': 711, '下午': 712, '吨数': 713, '这天': 714, '植': 715, '花': 716, '李老师': 717, '几个': 718, '电话机': 719, '水箱': 720, '一块': 721, '铁块': 722, '浸没': 723, '水面': 724, '纳税': 725, '每个': 726, '公民': 727, '义务': 728, '张老师': 729, '上个月': 730, '工资总额': 731, '按照': 732, '个人所得税法': 733, '有关': 734, '规定': 735, '部分': 736, '缴纳': 737, '个人所得税': 738, '应': 739, '合作': 740, '全班同学': 741, '颗': 742, '星': 743, '共运': 744, '每辆': 745, '故事': 746, '丙': 747, '丁': 748, '拿出': 749, '订购': 750, '分别': 751, '结算': 752, '付给': 753, '应付': 754, '倒数': 755, '减去': 756, '美元': 757, '兑换': 758, '人民币': 759, '投资': 760, '厂房': 761, '打字员': 762, '一部': 763, '书稿': 764, '千字': 765, '这部': 766, '王师傅': 767, '经': 768, '合格率': 769, '合格': 770, '城': 771, '再行': 772, '大米': 773, '星期': 774, '字典': 775, '定价': 776, '赚': 777, '这本': 778, '进价': 779, '钢管': 780, '转': 781, '上海': 782, '天津': 783, '距': 784, '图书馆': 785, '新': 786, '购进': 787, '文艺': 788, '百科全书': 789, '水渠': 790, '开挖': 791, '新华': 792, '全校学生': 793, '蛋糕': 794, '乘以': 795, '两艘': 796, '轮船': 797, '武汉': 798, '船': 799, '航道': 800, '解放军': 801, '汶川': 802, '灾区': 803, '架设': 804, '线路': 805, '侧面': 806, '展开': 807, '侧': 808, '扩建': 809, '教学楼': 810, '男': 811, '问': 812, '车轮': 813, '外': 814, '直径': 815, 'dm': 816, '滚动': 817, '张师傅': 818, '李师傅': 819, '某种': 820, '交纳': 821, '价值': 822, '税': 823, '公司': 824, '交税': 825, '铁丝': 826, '剪成': 827, '第一段': 828, '应该': 829, '过': 830, '收取': 831, '广告牌': 832, '这块': 833, '每平方米': 834, '师徒': 835, '徒弟': 836, '师傅': 837, '机械厂': 838, '过去': 839, '每班': 840, '写': 841, '含': 842, '瓶': 843, '含水': 844, '每人每天': 845, '一篇': 846, '字': 847, '厚': 848, '毫米': 849, '几本': 850, '厚度': 851, '行使': 852, '乐': 853, '强': 854, '几分钟': 855, '并': 856, '层': 857, '放': 858, '我': 859, '一层': 860, '猜': 861, '^': 862, '和小红': 863, '一起': 864, '玩': 865, '踢毽子': 866, '游戏': 867, '明说': 868, '踢': 869, '位': 870, '照片': 871, '加印': 872, '加上': 873, '出勤率': 874, '出勤': 875, '女工': 876, '玫瑰花': 877, '百合花': 878, '小巧': 879, '△': 880, '○': 881, '□': 882, '最大': 883, '步': 884, '第一周': 885, '两周': 886, '桥': 887, '村': 888, '明明': 889, '圆圆': 890, '多行': 891, '城市': 892, '装配': 893, '糖': 894, '分装': 895, '袋': 896, '养鸡场': 897, '星期一': 898, '收': 899, '鸡蛋': 900, '一箱': 901, '全年': 902, '级': 903, '仓库': 904, '存': 905, '放入': 906, '存粮': 907, '衣服': 908, '每件': 909, '美术组': 910, '体育组': 911, '去除': 912, '堆': 913, '煤': 914, '若': 915, '筑路': 916, '修筑': 917, '单独': 918, '布料': 919, '成本': 920, '技术革新': 921, '路上': 922, '一盏': 923, '路灯': 924, '吴': 925, '沿': 926, '以后': 927, '停': 928, '盏': 929, '动物园': 930, '狮子': 931, '恰好': 932, '小羊': 933, '计算器': 934, '电脑': 935, '艘': 936, '速度慢': 937, '切': 938, '使': 939, '并且': 940, '服装厂': 941, '服装': 942, '全': 943, '木块': 944, '正方体': 945, '例': 946, '步行': 947, '红光': 948, '产值': 949, '圆': 950, '半径': 951, '王大爷': 952, '稻谷': 953, '增产': 954, '养殖场': 955, '养': 956, '肉牛': 957, '终点': 958, '布袋': 959, '黑': 960, '球': 961, '手': 962, '摸': 963, '保证': 964, '红球': 965, '通过': 966, '座': 967, '大桥': 968, '作业': 969, '语文': 970, '这两项': 971, '时间': 972, '除数': 973, '被除数': 974, '建筑工地': 975, '红砖': 976, '两种': 977, '砖': 978, '平均数': 979, '第一个': 980, '丽': 981, '邮票': 982, '盒': 983, '每盒': 984, '每支': 985, '莉': 986, '水杯': 987, '找回': 988, '捐款': 989, '捐': 990, '一辆': 991, '捉': 992, '个人': 993, '起来': 994, '辆车': 995, '比例': 996, '项': 997, '一支': 998, '木': 999, '电杆': 1000, '支': 1001, '换成': 1002, '水泥': 1003, '世界': 1004, '之一': 1005, '陶俑': 1006, '其他': 1007, '果树': 1008, '一列': 1009, 'km': 1010, '间': 1011, '啄木鸟': 1012, '一天': 1013, '害虫': 1014, '青蛙': 1015, '亮': 1016, '育苗': 1017, '杯': 1018, '我校': 1019, '将': 1020, '召开': 1021, '运动会': 1022, '运动员': 1023, '号码': 1024, '布': 1025, '够': 1026, '够用': 1027, '统计': 1028, '户': 1029, '居民': 1030, '家用电器': 1031, '情况': 1032, '进行': 1033, '调查': 1034, '电器': 1035, '收录机': 1036, '彩电': 1037, '冰箱': 1038, '电冰箱': 1039, '每层': 1040, '自然': 1041, '角': 1042, '金鱼': 1043, '商品': 1044, '点心': 1045, '应得': 1046, '三年级': 1047, '参观': 1048, '科技馆': 1049, '每辆车': 1050, '据统计': 1051, '我国': 1052, '桦树': 1053, '全世界': 1054, '种类': 1055, '什么': 1056, '编写': 1057, '《': 1058, '学': 1059, '》': 1060, '出版': 1061, '取得': 1062, '稿费': 1063, '收入': 1064, '税率': 1065, '缴': 1066, '含糖': 1067, '捆': 1068, '电线': 1069, '练习': 1070, '林场': 1071, '校服': 1072, '小学生': 1073, '装运': 1074, '钢材': 1075, '节车厢': 1076, '敏': 1077, '农药': 1078, 'kg': 1079, '加水': 1080, '稀释': 1081, '喷洒': 1082, '化肥': 1083, '袋装': 1084, '同一': 1085, '地点': 1086, '相反': 1087, '方向': 1088, '一班': 1089, '废品': 1090, '两队': 1091, '容纳': 1092, '立方米': 1093, '水中': 1094, '多用': 1095, '东方': 1096, '果汁': 1097, '柴油': 1098, '园林': 1099, '梧桐树': 1100, '因数': 1101, '两根': 1102, '份': 1103, '录入': 1104, '彩车': 1105, '排成一列': 1106, '相隔': 1107, '彩': 1108, '车队': 1109, '水彩笔': 1110, '绘画': 1111, '纸': 1112, '每块': 1113, '工程': 1114, '筐': 1115, '单人': 1116, '课桌椅': 1117, '哥哥': 1118, '大学': 1119, '平均速度': 1120, '坐火车': 1121, '缩小': 1122, '变为': 1123, '庆祝': 1124, '周年': 1125, '舞蹈队': 1126, '合唱队': 1127, '鸭蛋': 1128, '请问': 1129, '蛋': 1130, '已修': 1131, '啤酒': 1132, '升': 1133, '一瓶': 1134, '该': 1135, '玻璃球': 1136, '！': 1137, '说': 1138, '要是': 1139, '菜场': 1140, '黄瓜': 1141, '某次': 1142, '倒扣': 1143, '此次': 1144, '变成': 1145, '蒸发掉': 1146, '战士': 1147, '淡水': 1148, '问题': 1149, '修建': 1150, 'm': 1151, '蓄水池': 1152, '土': 1153, '往': 1154, '深': 1155, '市场': 1156, '笔记本电脑': 1157, '市': 1158, '多长时间': 1159, '朵花': 1160, '朵': 1161, '扎成': 1162, '一束': 1163, '些': 1164, '分给': 1165, '束': 1166, '煤矿': 1167, '卡车': 1168, '装有': 1169, '卡': 1170, '人人': 1171, '订阅': 1172, '报纸': 1173, '订': 1174, '少年报': 1175, '报': 1176, '菜': 1177, '白菜': 1178, '推出': 1179, '每月': 1180, '月租费': 1181, '然后': 1182, '电话': 1183, '付费': 1184, '以上': 1185, '丝带': 1186, '段长': 1187, '这根': 1188, '记者': 1189, '假期': 1190, '举行': 1191, '一项': 1192, '结': 1193, '两端': 1194, '小熊猫': 1195, '鞋子': 1196, '双': 1197, '消耗': 1198, '汽油': 1199, '蜗牛': 1200, '沿着': 1201, '往上爬': 1202, '上爬': 1203, '下滑': 1204, '像': 1205, '老鼠': 1206, '猫': 1207, '合': 1208, '环形': 1209, '跑道': 1210, '背向': 1211, '飞': 1212, '借书': 1213, '上山': 1214, '下山': 1215, '收集': 1216, '飞机': 1217, '家住': 1218, '那': 1219, '楼房': 1220, '楼梯': 1221, '楼': 1222, '今天': 1223, '只有': 1224, '回家': 1225, '一楼': 1226, '购回': 1227, '香蕉': 1228, '象棋': 1229, '书法': 1230, '鱼': 1231, '店': 1232, '为什么': 1233, '连': 1234, '加': 1235, '盈利': 1236, '额': 1237, '母亲': 1238, '月工资': 1239, '工资收入': 1240, '余额': 1241, '冬': 1242, '做一套': 1243, '用布': 1244, '•': 1245, '节': 1246, '济南': 1247, '商场': 1248, '促销': 1249, '提高': 1250, '销售': 1251, '王奶奶': 1252, '用电': 1253, '度': 1254, '节约用电': 1255, '多少度': 1256, '绳': 1257, '节约': 1258, '用水': 1259, '两座': 1260, '部': 1261, '集': 1262, '播放': 1263, '每集': 1264, '交费': 1265, '交': 1266, '这件': 1267, '农具': 1268, '玩具厂': 1269, '兔': 1270, '材料': 1271, '改进': 1272, '操场上': 1273, '口袋': 1274, '年级': 1275, '一位': 1276, '玩具': 1277, '仍': 1278, '只好': 1279, '售出': 1280, '这位': 1281, '天运': 1282, '萝卜': 1283, '西红柿': 1284, '除': 1285, '上楼': 1286, '台阶': 1287, '一层楼': 1288, '练习本': 1289, '无': 1290, '剩余': 1291, '贵': 1292, '制造厂': 1293, '遇到': 1294, '不停': 1295, '往返': 1296, '于': 1297, '进': 1298, '班上': 1299, '磨': 1300, '面粉': 1301, '广州': 1302, '离': 1303, '点': 1304, '处': 1305, '缺水': 1306, '根据': 1307, '中国': 1308, '水资源': 1309, '人均': 1310, '仅': 1311, '爷爷': 1312, '家养': 1313, '年产': 1314, '一年': 1315, '粮库': 1316, '大卡车': 1317, '熊猫': 1318, '乘数': 1319, '排成': 1320, '边': 1321, '方阵': 1322, '奶奶': 1323, '菜市场': 1324, '茄子': 1325, '付出': 1326, '橙汁': 1327, '罐': 1328, '付款': 1329, '灰': 1330, '太': 1331, '狼': 1332, '枚': 1333, '玩具店': 1334, '育才': 1335, '日': 1336, '本息': 1337, '盆花': 1338, '盆': 1339, '换': 1340, '女同学': 1341, '男同学': 1342, '团体操': 1343, '表演': 1344, '一行': 1345, '几行': 1346, '开工': 1347, '能够': 1348, '某校': 1349, '派出': 1350, '每队': 1351, '全市': 1352, '奥数': 1353, '平均分': 1354, '航模': 1355, '两条': 1356, '调': 1357, '棋子': 1358, '粉笔': 1359, '学校食堂': 1360, '我们': 1361, '本月': 1362, '彩色': 1363, '容积': 1364, '注水': 1365, '注满': 1366, '水池': 1367, '体育': 1368, '达标': 1369, '测试': 1370, '达到': 1371, '优秀': 1372, '黑天鹅': 1373, '白天鹅': 1374, '粮店': 1375, '边长': 1376, '湖边': 1377, '白鹤': 1378, '宝': 1379, '起': 1380, '省': 1381, '高速公路': 1382, '第一期': 1383, '期': 1384, '跳高': 1385, '欢欢': 1386, '红红': 1387, '低': 1388, '强强': 1389, '拖拉机': 1390, '改用': 1391, '展出': 1392, '标本': 1393, '排': 1394, '放满': 1395, '几块': 1396, '可乐': 1397, '喝': 1398, '英语': 1399, '教师': 1400, '装修': 1401, '粮': 1402, '写成': 1403, '分数': 1404, '一家': 1405, '旅游': 1406, '目的地': 1407, '称': 1408, '两块': 1409, '小麦': 1410, '第一块': 1411, '印刷厂': 1412, '上半年': 1413, '支出': 1414, '下半年': 1415, '纯收入': 1416, '河边': 1417, '某人': 1418, '摩托车': 1419, '甲班': 1420, '乙班': 1421, '美国': 1422, '汇': 1423, '回': 1424, '小轿车': 1425, '她家': 1426, '今天上午': 1427, '两只': 1428, '兔子': 1429, '小兔': 1430, '平': 1431, '踢毽': 1432, '云': 1433, '扩大': 1434, '童话故事': 1435, '这辆': 1436, '艺术': 1437, '全校': 1438, '白': 1439, '可能性': 1440, '奶奶家': 1441, '乘车': 1442, '新世纪': 1443, '百货': 1444, '庆': 1445, '降': 1446, '这项': 1447, '果品': 1448, '损耗': 1449, '希望': 1450, '利润': 1451, '帮': 1452, '营业员': 1453, '零售价': 1454, '书包': 1455, '头上': 1456, '车尾': 1457, '小鸡': 1458, '水电费': 1459, '千瓦时': 1460, '香皂': 1461, '再生产': 1462, '目前': 1463, '次数': 1464, '它们': 1465, '批发': 1466, '司机': 1467, '液晶电视': 1468, '运送': 1469, '运到': 1470, '运货': 1471, '北京': 1472, '喜欢': 1473, '打乒乓球': 1474, '羽毛球': 1475, '运动': 1476, '松': 1477, '黄河': 1478, '号': 1479, '货轮': 1480, '港': 1481, '航行': 1482, '舞蹈': 1483, '大楼': 1484, '栽树': 1485, '相邻': 1486, '两棵树': 1487, '儿童节': 1488, '新华书店': 1489, '图书': 1490, '美': 1491, '售货员': 1492, '绕': 1493, '后面': 1494, '一车': 1495, '蔬菜': 1496, '送到': 1497, '盏灯': 1498, '好': 1499, '长方形': 1500, '小车': 1501, '大车': 1502, '游乐园': 1503, '解答': 1504, '简分数': 1505, '分子': 1506, '分母': 1507, '直角三角形': 1508, '长度': 1509, '一款': 1510, '毛衣': 1511, '接待': 1512, '游客': 1513, '昨天': 1514, '用于': 1515, '费用': 1516, '一年级': 1517, '鸭子': 1518, '含盐': 1519, '童装': 1520, '化成': 1521, '百分数': 1522, '两组': 1523, '秒': 1524, '离开': 1525, '最大公约数': 1526, '最小': 1527, '公倍数': 1528, '现': 1529, '除法': 1530, '相乘': 1531, '道路': 1532, '上坡': 1533, '下坡': 1534, '兰兰': 1535, '人均收入': 1536, '就是': 1537, '值': 1538, '增长': 1539, '两批': 1540, '电影': 1541, '第一批': 1542, '分成': 1543, '每组': 1544, '烧煤': 1545, '量': 1546, '烧': 1547, '一个月': 1548, '服装店': 1549, '酬宾': 1550, '一律': 1551, '小张': 1552, '反向而行': 1553, '读': 1554, '心跳': 1555, '大象': 1556, '师生': 1557, '区': 1558, '郊游': 1559, '乘坐': 1560, '测验': 1561, '一道': 1562, '两道': 1563, '报名': 1564, '粉刷': 1565, '墙壁': 1566, '涂料': 1567, '排队': 1568, '迎': 1569, '搬': 1570, '一堆': 1571, '青海': 1572, '发生': 1573, '地震': 1574, '白兔': 1575, '酒精': 1576, '现有': 1577, '竹竿': 1578, '插入': 1579, '池': 1580, '露出': 1581, '插': 1582, '泥': 1583, '杭州': 1584, '先行': 1585, '鞋': 1586, '销售额': 1587, '所有': 1588, '皮鞋': 1589, '宣传': 1590, '资金': 1591, '优惠': 1592, '一双': 1593, '标价': 1594, '成活率': 1595, '活': 1596, '欣': 1597, '试验田': 1598, '土豆': 1599, '一头': 1600, '一棵': 1601, '寿命': 1602, '一般': 1603, '饲养': 1604, '玫瑰': 1605, '杨': 1606, '利息税': 1607, '获得': 1608, '税后': 1609, '饲养场': 1610, '公鸡': 1611, '母鸡': 1612, '笼': 1613, '装满': 1614, '笔记本': 1615, '看成': 1616, '于是': 1617, '正确': 1618, '普通': 1619, '高速': 1620, '万件': 1621, '产量': 1622, '最少': 1623, '票': 1624, '门票': 1625, '全天': 1626, '数学考试': 1627, '分为': 1628, '及格': 1629, '不及格': 1630, '缩短': 1631, '课': 1632, '每位': 1633, '每节课': 1634, '继续前进': 1635, '宾馆': 1636, '外国': 1637, '昆明': 1638, '植物园': 1639, '中午': 1640, '岁': 1641, '年龄': 1642, '多少岁': 1643, '养鸡': 1644, '专业户': 1645, '陈老师': 1646, '包': 1647, '一包': 1648, '供水': 1649, '不足': 1650, '严重': 1651, '截': 1652, '南': 1653, '前年': 1654, '橘树': 1655, '消费': 1656, '减少': 1657, '乒乓球': 1658, '手帕': 1659, '采用': 1660, '食物': 1661, '工资': 1662, '某市': 1663, '调出': 1664, '件数': 1665, '山羊': 1666, '燕子': 1667, '起点': 1668, '那天': 1669, '打折': 1670, '每份': 1671, '两班': 1672, '葡萄': 1673, '各自': 1674, '白天': 1675, '夜里': 1676, '井口': 1677, '客轮': 1678, '搞': 1679, '一段路': 1680, '伯伯': 1681, '桌面': 1682, '早': 1683, '汽车厂': 1684, '小汽车': 1685, '装完': 1686, '安装': 1687, '平原': 1688, '丰收': 1689, '中心': 1690, '少年宫': 1691, '幅': 1692, '画': 1693, '瓶子': 1694, '幢': 1695, '每户': 1696, '这幢': 1697, '春季': 1698, '聪': 1699, '批发市场': 1700, '牛肉': 1701, '含量': 1702, '含有': 1703, '大衣': 1704, '贵宾卡': 1705, '糖果': 1706, '加入': 1707, '采集': 1708, '树种': 1709, '葡萄糖': 1710, '蜂蜜': 1711, '铺': 1712, '草坪': 1713, '铅笔盒': 1714, '总和': 1715, '课外': 1716, '捡': 1717, '田径队': 1718, '外婆家': 1719, '走法': 1720, '少运': 1721, '脚': 1722, '熊': 1723, '方案': 1724, '投进': 1725, '货': 1726, '停车场': 1727, '放在': 1728, '文具盒': 1729, '粮站': 1730, '牛奶': 1731, '建筑队': 1732, '铺路': 1733, '木头': 1734, '袜子': 1735, '回来': 1736, '衬衫': 1737, '两站': 1738, '书店': 1739, '购书': 1740, '买下': 1741, '大豆': 1742, '出油率': 1743, '至': 1744, '最高': 1745, '运往': 1746, '获利': 1747, '围成': 1748, '圆柱形': 1749, '纸筒': 1750, '接头处': 1751, '不计': 1752, '国庆节': 1753, '栏杆': 1754, '彩旗': 1755, '面': 1756, '中间': 1757, '圆形': 1758, '花园': 1759, '周围': 1760, '节水': 1761, '图': 1762, '聪聪': 1763, '菊花': 1764, '月季花': 1765, '外地': 1766, '倒入': 1767, '容量': 1768, '植树节': 1769, '纺织厂': 1770, '男工': 1771, '彩带': 1772, '包装': 1773, '花束': 1774, '英': 1775, '送给': 1776, '著名': 1777, '课外书': 1778, '铺设': 1779, '地球': 1780, '物体': 1781, '月球': 1782, '乐乐': 1783, '洗衣机': 1784, '运来了': 1785, '节电池': 1786, '一盒': 1787, '盒装': 1788, '截去': 1789, '一部分': 1790, '少年': 1791, '菠萝': 1792, '期末考试': 1793, '成绩': 1794, '自己': 1795, '玲玲': 1796, '大王': 1797, '芒果': 1798, '春': 1799, '招进': 1800, '一座': 1801, '一半': 1802, '室': 1803, '买进': 1804, '做操': 1805, '前面': 1806, '客': 1807, '会议室': 1808, '方砖': 1809, '一枝': 1810, '车上': 1811, '乘客': 1812, '轿车': 1813, '袋子': 1814, '大小': 1815, '黄': 1816, '袋中': 1817, '任意': 1818, '农场': 1819, '耕地': 1820, '耕': 1821, '加价': 1822, '猎豹': 1823, '最快': 1824, '时速': 1825, '马': 1826, '奶糖': 1827, '军舰': 1828, '海水': 1829, '逆水': 1830, '外语': 1831, '懂': 1832, '俄语': 1833, '一片': 1834, '阔叶林': 1835, '制造': 1836, '氧气': 1837, '片': 1838, '圆珠笔': 1839, '何': 1840, '高处': 1841, '下落': 1842, '地面': 1843, '弹起': 1844, '高度': 1845, '每包': 1846, '均': 1847, '报刊': 1848, '学习': 1849, '秋游': 1850, '车票': 1851, '家里': 1852, '姐姐': 1853, '正在': 1854, '科普': 1855, '工地': 1856, '黄沙': 1857, '数学课': 1858, '操作': 1859, '百': 1860, '福': 1861, '体育锻炼': 1862, '标准': 1863, '苗苗': 1864, '笑笑': 1865, '猴子': 1866, '桃': 1867, '桃子': 1868, '猴': 1869, '批改': 1870, '篇': 1871, '作文': 1872, '被': 1873, '布鞋': 1874, '提速': 1875, '可行': 1876, '车辆': 1877, '存款': 1878, '每棵': 1879, '作': 1880, '亿': 1881, '人口': 1882, '握': 1883, '表面积': 1884, '海洋': 1885, '鱼类': 1886, '石料': 1887, '梯形': 1888, '摆放': 1889, '下层': 1890, '两层': 1891, '工作人员': 1892, '精简': 1893, '上旬': 1894, '炼': 1895, '中旬': 1896, '光': 1897, '梅': 1898, '家电': 1899, '电扇': 1900, '缺勤': 1901, '甲组': 1902, '乙组': 1903, '小数': 1904, '弟弟': 1905, '利率': 1906, '期中考试': 1907, '科': 1908, '书架': 1909, '梅花鹿': 1910, '长颈鹿': 1911, '矮': 1912, '幼儿园': 1913, '发': 1914, '发给': 1915, '留': 1916, '备用': 1917, '榨出': 1918, '营业额': 1919, '实际收入': 1920, '一点': 1921, '一直': 1922, '节约用水': 1923, '产': 1924, '寒假': 1925, '单': 1926, '向阳': 1927, '纵队': 1928, '前后': 1929, '队伍': 1930, '伦敦': 1931, '奥运会': 1932, '奖牌': 1933, '金牌': 1934, '银': 1935, '混合': 1936, '制作': 1937, '表面': 1938, '总重量': 1939, '填': 1940, '阳光': 1941, '操场': 1942, '一面': 1943, '漫画书': 1944, '举办': 1945, '兰花': 1946, '花坛': 1947, '郁金香': 1948, '苹果园': 1949, '迎接': 1950, '新年': 1951, '红花': 1952, '绿': 1953, '一笔': 1954, '按规定': 1955, '会': 1956, '吹': 1957, '利民': 1958, '农业': 1959, '园': 1960, '门': 1961, '总分': 1962, '油桶': 1963, '游泳池': 1964, '占地面积': 1965, '假如': 1966, '出粉率': 1967, '新建': 1968, '配备': 1969, '配': 1970, '下乡': 1971, '农民': 1972, '凡': 1973, '享受': 1974, '政府': 1975, '补贴': 1976, '节省': 1977, '新书': 1978, '书架上': 1979, '玻璃杯': 1980, '水深': 1981, '圆柱体': 1982, '露': 1983, '不会': 1984, '小区': 1985, '大树': 1986, '干': 1987, '树干': 1988, '丽丽': 1989, '绵羊': 1990, '羊': 1991, '一场': 1992, '场': 1993, '圆锥': 1994, '立方分米': 1995, '等': 1996, '星星': 1997, '制衣厂': 1998, '裁剪': 1999, '方法': 2000, '育英': 2001, '体操队': 2002, '抹': 2003, '博物馆': 2004, '新学期': 2005, '牌': 2006, '淘气': 2007, '看中': 2008, '分期付款': 2009, '现金': 2010, '一次性': 2011, '算了': 2012, '一下': 2013, '方式': 2014, '用品商店': 2015, '处理': 2016, '期间': 2017, '一月份': 2018, '总收入': 2019, '超出': 2020, '缴税': 2021, 't': 2022, '复查': 2023, '误': 2024, '街心公园': 2025, '各种': 2026, '槐树': 2027, '学习用品': 2028, '花店': 2029, '布置': 2030, '数学题': 2031, '容器': 2032, '注入': 2033, '县城': 2034, '庄': 2035, '外出': 2036, '春游': 2037, '安徒生': 2038, '美化': 2039, '改': 2040, '游玩': 2041, '亚': 2042, '大家': 2043, '小林': 2044, '东家': 2045, '修补': 2046, '钢筋': 2047, '红星': 2048, '编': 2049, '一组': 2050, '酸奶': 2051, '每瓶': 2052, '列式': 2053, '救灾物资': 2054, '良好': 2055, '黄花': 2056, '玲': 2057, '因为': 2058, '成都': 2059, '相': 2060, '背': 2061, '池塘': 2062, '民': 2063, '几只': 2064, '锯下': 2065, '几米': 2066, '篇文章': 2067, '每页': 2068, '他家': 2069, '小猪': 2070, '轮子': 2071, '广播操': 2072, '考': 2073, '麦地': 2074, '国庆': 2075, '中央': 2076, '国际': 2077, '总长': 2078, '捐赠': 2079, '进度': 2080, '架': 2081, '比值': 2082, '前项': 2083, '一旁': 2084, '演讲': 2085, '已有': 2086, '棱': 2087, '钢': 2088, '锻造': 2089, '横截面': 2090, '港口': 2091, '中途': 2092, '上船': 2093, '收获': 2094, '矿泉水': 2095, '甘蔗': 2096, '叫': 2097, '设': 2098, '耗油': 2099, '表示': 2100, '加数': 2101, '游乐场': 2102, '离去': 2103, '中介': 2104, '顾客': 2105, '房屋': 2106, '中介费': 2107, '李先生': 2108, '契税': 2109, '一袋': 2110, '进货价': 2111, '技术': 2112, '长跑': 2113, '训练': 2114, '上周': 2115, '存期': 2116, '上升': 2117, '足球队': 2118, '篮球队': 2119, '借出': 2120, '镇': 2121, '时候': 2122, '空车': 2123, '万': 2124, '四川': 2125, '应从': 2126, '奖': 2127, '奖金': 2128, '总额': 2129, '这次': 2130, '预计': 2131, '头牛': 2132, '牛': 2133, '打算': 2134, '实际上': 2135, '原定': 2136, '卖价': 2137, '基础': 2138, '沙子': 2139, '格林童话': 2140, '几岁': 2141, '彤': 2142, '数学组': 2143, '拍': 2144, '剪下': 2145, '林': 2146, '游': 2147, '件产品': 2148, '爬楼梯': 2149, '税法': 2150, '帽子': 2151, '奥林匹克': 2152, '层楼': 2153, '多少级': 2154, '接着': 2155, '田径比赛': 2156, '选手': 2157, '添': 2158, '春节': 2159, '小队': 2160, '优惠政策': 2161, '几吨': 2162, '蓝花': 2163, '家具': 2164, '不能': 2165, '喜洋洋': 2166, '体育场': 2167, '赛跑': 2168, '一处': 2169, '睡觉': 2170, '领先': 2171, '落后': 2172, '考试': 2173, '化肥厂': 2174, '宽比': 2175, '长短': 2176, '新星': 2177, '第一季度': 2178, '课外读物': 2179, '两筐': 2180, '图书室': 2181, '行军': 2182, '水槽': 2183, '石块': 2184, '水位': 2185, '下降': 2186, '拉': 2187, '面包': 2188, '几箱': 2189, '三角形': 2190, '每周': 2191, '需用': 2192, '拿走': 2193, '画画': 2194, '世博会': 2195, '安全': 2196, '%': 2197, '房间': 2198, '货款': 2199, '某村': 2200, '花生': 2201, '多种': 2202, '水管': 2203, '儿童服装': 2204, '所用': 2205, '成活': 2206, '及': 2207, '车速': 2208, '北京市': 2209, '鸡场': 2210, '肉鸡': 2211, '生物': 2212, '数字': 2213, '位数': 2214, '小红花': 2215, '省城': 2216, '晚上': 2217, '卖完': 2218, '种树': 2219, '从中': 2220, '顺水': 2221, '体内': 2222, '水分': 2223, '酿': 2224, '开学': 2225, '按计划': 2226, '结束': 2227, '板': 2228, '巧克力': 2229, '营业税': 2230, '运动装': 2231, '队员': 2232, '笔': 2233, '小花': 2234, '涛': 2235, '房子': 2236, '敲': 2237, '当时': 2238, '钟': 2239, '压岁钱': 2240, '捐给': 2241, '希望工程': 2242, '地砖': 2243, '落': 2244, '福娃': 2245, '满': 2246, '看到': 2247, '骑车': 2248, '得数': 2249, '馆': 2250, '管': 2251, '几页': 2252, '驾驶': 2253, '总量': 2254, '球类': 2255, '商都': 2256, '软盘': 2257, '卖掉': 2258, '玩具车': 2259, '布娃娃': 2260, '少卖': 2261, '天才': 2262, '借给': 2263, '大扫除': 2264, '擦': 2265, '夏令营': 2266, '画片': 2267, '夏季': 2268, '药粉': 2269, '自': 2270, '杯子': 2271, '空瓶': 2272, '杯水': 2273, '木条': 2274, '球迷': 2275, '文学': 2276, '册': 2277, '结成': 2278, '冰': 2279, '纸盒': 2280, '互为': 2281, '捐书': 2282, '旺': 2283, '梨子': 2284, '筑': 2285, '凳子': 2286, '玉米': 2287, '涂': 2288, '看书': 2289, '墨水': 2290, '水路': 2291, '科学': 2292, '爱好者': 2293, '该校': 2294, '作品': 2295, '第一组': 2296, '若干名': 2297, '电饭煲': 2298, '[': 2299, ']': 2300, '小孩': 2301, '养鱼': 2302, '尾': 2303, '法': 2304, '万个': 2305, '丛书': 2306, '水果糖': 2307, '水泥厂': 2308, '青岛': 2309, '营': 2310, '建': 2311, '男女': 2312, '员工': 2313, '女': 2314, '简便': 2315, '架飞机': 2316, '文稿': 2317, '蓝鲸': 2318, '割草': 2319, '割': 2320, '组装': 2321, '搬运': 2322, '西装': 2323, '家庭': 2324, '展览': 2325, '改为': 2326, '大客车': 2327, '水壶': 2328, '装订': 2329, '栽种': 2330, '专卖店': 2331, '电影院': 2332, '座位': 2333, '每排': 2334, '观看': 2335, '分行': 2336, '收割': 2337, '麦田': 2338, '摆': 2339, '完工': 2340, '木棒': 2341, '通风管': 2342, '铁皮': 2343, '运动服': 2344, '打印': 2345, '文章': 2346, '前一天': 2347, '住院': 2348, '体重减轻': 2349, '贴': 2350, '商标纸': 2351, '原料': 2352, '影碟机': 2353, '进来': 2354, '语': 2355, '花园里': 2356, '红玫瑰': 2357, '彩纸': 2358, '几张': 2359, '一杯': 2360, '猪': 2361, '小胖': 2362, '碗': 2363, '超产': 2364, '白色': 2365, '塑料袋': 2366, '周末': 2367, '车站': 2368, '摘': 2369, '选': 2370, '教育': 2371, '献爱心': 2372, '办事': 2373, '万吨': 2374, '载重量': 2375, '集装箱': 2376, '方方': 2377, '橙子': 2378, '粉': 2379, '白糖': 2380, '期末': 2381, '胜利': 2382, '抽水机': 2383, '空': 2384, '碰坏': 2385, '只能': 2386, '宇': 2387, '暑假': 2388, '体操': 2389, '位置': 2390, '左': 2391, '右': 2392, '从前': 2393, '码头': 2394, '海里': 2395, '青菜': 2396, '两家': 2397, '家中': 2398, '两袋': 2399, '调进': 2400, '阶梯教室': 2401, '演出': 2402, '排座位': 2403, '其': 2404, '此': 2405, '成本价': 2406, '客厅': 2407, '中学': 2408, '进入': 2409, '直到': 2410, '无盖': 2411, '体育馆': 2412, '减': 2413, '金牌数': 2414, '手机': 2415, '市民': 2416, '几棵': 2417, '每枝': 2418, '商品房': 2419, '房价': 2420, '实行': 2421, '保险': 2422, '绿色': 2423, '万棵': 2424, '小海': 2425, '绿化': 2426, '元旦': 2427, '少先队员': 2428, '女队员': 2429, '全体': 2430, '男队员': 2431, '小班': 2432, '块冰': 2433, '多重': 2434, '磁悬浮列车': 2435, '沙坑': 2436, '测': 2437, '星光': 2438, '收割机': 2439, '购置税': 2440, '织布': 2441, '织': 2442, '减价': 2443, '宏': 2444, '鞋厂': 2445, '储蓄': 2446, '人体': 2447, '血液': 2448, '套书': 2449, '让': 2450, '独': 2451, '爱': 2452, '抗震救灾': 2453, '医生': 2454, '育': 2455, '达标率': 2456, '超额': 2457, '装箱': 2458, 'B型': 2459, '开幕式': 2460, '游泳': 2461, '销售量': 2462, '乙丙': 2463, '届': 2464, '伟': 2465, '文件': 2466, '买菜': 2467, '荤菜': 2468, '素菜': 2469, '两样': 2470, '钢琴': 2471, '宁': 2472, '全长约': 2473, '全家': 2474, '鱼缸': 2475, '地方': 2476, '必须': 2477, '合唱组': 2478, '登山': 2479, '立即': 2480, '上下': 2481, '等腰三角': 2482, '形': 2483, '腰': 2484, '除了': 2485, '收回': 2486, '合金': 2487, '锡': 2488, '跳远': 2489, '红领巾': 2490, '燕': 2491, '小旗': 2492, '用来': 2493, '河': 2494, '连环画': 2495, '小说': 2496, '施工': 2497, '开通': 2498, '爱心': 2499, '艺术节': 2500, '节目': 2501, '合唱': 2502, '选出': 2503, '商家': 2504, '过年': 2505, '毫升': 2506, '到校': 2507, '日记本': 2508, '太阳': 2509, '两位': 2510, '面包车': 2511, '植树造林': 2512, '造林': 2513, '﹖': 2514, '约定': 2515, '纸片': 2516, '台数': 2517, '影院': 2518, '放映': 2519, '电影票': 2520, '观众': 2521, '种颜色': 2522, '眼睛': 2523, '每种': 2524, '颜色': 2525, '全场': 2526, '几元': 2527, '用纸': 2528, '儿子': 2529, '俩': 2530, '装满水': 2531, '花瓶': 2532, '员': 2533, '利': 2534, '打破': 2535, '一副': 2536, '乒乓球拍': 2537, '王大妈': 2538, '黄牛': 2539, '总共': 2540, '住': 2541, '液晶': 2542, '显示器': 2543, '万台': 2544, '总产量': 2545, '杏树': 2546, '十一': 2547, '黄金周': 2548, '达': 2549, '电视塔': 2550, '半圆': 2551, '记': 2552, '正': 2553, '计算机': 2554, '共存': 2555, '饭店': 2556, '上学': 2557, '两辆车': 2558, '文具店': 2559, '天天': 2560, '放到': 2561, '色': 2562, '练习册': 2563, '知': 2564, '种子': 2565, '发芽率': 2566, '播种': 2567, '粒': 2568, '可能': 2569, '发芽': 2570, '掉': 2571, '冬冬': 2572, '送货': 2573, '相除': 2574, '决定': 2575, '街道': 2576, '樟树': 2577, '广玉兰': 2578, '油菜籽': 2579, '小龙': 2580, '食品店': 2581, '木板': 2582, '培训': 2583, '全国': 2584, '两项': 2585, '获奖': 2586, '虎': 2587, '复读机': 2588, '毛巾': 2589, '乡': 2590, '工业': 2591, '总产值': 2592, '炉灶': 2593, '好吃': 2594, '画册': 2595, '公斤': 2596, '化': 2597, '物资': 2598, '山区': 2599, '出生': 2600, '劳动': 2601, '一艘': 2602, '单位': 2603, '副': 2604, '手套': 2605, '运动鞋': 2606, '排头': 2607, '数起': 2608, '长大': 2609, '生长': 2610, '奖品': 2611, '明年': 2612, '寄': 2613, '邮件': 2614, '裙子': 2615, '爬山': 2616, '因': 2617, '自行车厂': 2618, '西': 2619, '东西': 2620, '套装': 2621, '油箱': 2622, '齿轮': 2623, '小齿轮': 2624, '齿': 2625, '一倍': 2626, '形状': 2627, '汽水': 2628, '几时': 2629, '两本书': 2630, '卖出去': 2631, '春天': 2632, '县': 2633, '万册': 2634, '捐献': 2635, '栋': 2636, '每栋': 2637, '人家': 2638, '制': 2639, '维修': 2640, '蓝球': 2641, '票价': 2642, '打扫': 2643, '刘老师': 2644, '文': 2645, '建筑面积': 2646, '电脑公司': 2647, '风': 2648, '两台': 2649, '花费': 2650, '一分钟': 2651, '静': 2652, '棉花': 2653, '茶叶': 2654, '唱歌': 2655, '一小': 2656, '某个': 2657, '不但': 2658, '赔偿': 2659, '采': 2660, '蜜': 2661, '凉鞋': 2662, '皮': 2663, '山地': 2664, '狗': 2665, '回头': 2666, '奔跑': 2667, '红色': 2668, '修理': 2669, '录音机': 2670, '个人收入': 2671, '征收': 2672, '汉口': 2673, '电视': 2674, '上网': 2675, '化工厂': 2676, '小说书': 2677, '小兵': 2678, '支援灾区': 2679, '义卖': 2680, '作为': 2681, '底部': 2682, '幸福': 2683, '西瓜': 2684, '荣': 2685, '一筐': 2686, '前进': 2687, '晓': 2688, '零用钱': 2689, '礼物': 2690, '药液': 2691, '手表': 2692, '等差数列': 2693, '项目': 2694, '收费': 2695, '调整': 2696, '少付': 2697, '微波炉': 2698, '易拉罐': 2699, '借': 2700, '全班学生': 2701, '东城': 2702, '西城': 2703, '新村': 2704, '红旗': 2705, '决赛': 2706, '几次': 2707, '税款': 2708, '保险费率': 2709, '保险费': 2710, '答对': 2711, '答错': 2712, '煤气管道': 2713, '木棍': 2714, '重叠': 2715, '百货商店': 2716, '售': 2717, '光华': 2718, '衬衣': 2719, '书本': 2720, '馒头': 2721, '晶': 2722, '批发价': 2723, '机床厂': 2724, '车床': 2725, '动物': 2726, '野生': 2727, '丹顶鹤': 2728, '其它': 2729, '破损': 2730, '上涨': 2731, '首付': 2732, '低于': 2733, '贷款': 2734, '总价': 2735, '气球': 2736, '完全相同': 2737, '涂成': 2738, '蓝色': 2739, '朝': 2740, '白布': 2741, '花布': 2742, '亩': 2743, '亩产': 2744, '食用油': 2745, '大字': 2746, '上层': 2747, '取': 2748, '一架': 2749, '多页': 2750, '盘子': 2751, '吃掉': 2752, '直角': 2753, '妹妹': 2754, '穿过': 2755, '毛笔': 2756, '读物': 2757, '鲸鱼': 2758, '接': 2759, '接口处': 2760, '以前': 2761, '只要': 2762, '朋友': 2763, '科普读物': 2764, '册数': 2765, '小船': 2766, '载重': 2767, '大船': 2768, '健康': 2769, '声音': 2770, '打碎': 2771, '埋': 2772, '地下': 2773, '赵': 2774, '佳': 2775, 'C': 2776, '空地': 2777, '相加': 2778, '养鸭': 2779, '纸花': 2780, '重新': 2781, '看过': 2782, '截下': 2783, '驶过': 2784, '猎狗': 2785, '鸵鸟': 2786, '抽': 2787, '垃圾': 2788, '练': 2789, '过河': 2790, '载': 2791, '改造': 2792, '扇形': 2793, '统计图': 2794, '榨': 2795, '帮助': 2796, '校': 2797, '楼下': 2798, '小米': 2799, '芝麻': 2800, '沪宁': 2801, '电费': 2802, '支援': 2803, '玉树': 2804, '赶制': 2805, '顶': 2806, '零花钱': 2807, '另外': 2808, '得分': 2809, '而且': 2810, '回去': 2811, '多出': 2812, '米处': 2813, '机床': 2814, '折': 2815, '价': 2816, '价格比': 2817, '接到': 2818, '万套': 2819, '水桶': 2820, '部手机': 2821, '草地': 2822, '女装': 2823, '不是': 2824, '直': 2825, '为此': 2826, '该班': 2827, '未达标': 2828, '几组': 2829, '社团': 2830, '工艺': 2831, '改变': 2832, '改成': 2833, '体检': 2834, '冠军': 2835, '再剪': 2836, '游泳队': 2837, '施工队': 2838, '石头': 2839, '且': 2840, '按时': 2841, '购': 2842, '一捆': 2843, '欧': 2844, '不断': 2845, '来回': 2846, '为止': 2847, '厂家': 2848, '肥皂': 2849, '坐满': 2850, '还是': 2851, 'a': 2852, 'b': 2853, '检测': 2854, '优': 2855, '良': 2856, '天气': 2857, '上面': 2858, '乐器': 2859, '参考书': 2860, '辉': 2861, '工作效率': 2862, '每步': 2863, '用户': 2864, '用电量': 2865, '大白菜': 2866, '溜溜球': 2867, '畜牧场': 2868, '头数': 2869, '手工': 2870, '农机': 2871, '买票': 2872, '阅览室': 2873, '来看': 2874, '剪掉': 2875, '山路': 2876, '融化': 2877, '收音机': 2878, '塔': 2879, '两堆': 2880, '石子': 2881, '买回来': 2882, '词典': 2883, '木桩': 2884, '泥土': 2885, '圆锥体': 2886, '羽毛球拍': 2887, '检阅': 2888, '海象': 2889, '驶向': 2890, '耽误': 2891, '瑞安': 2892, '江': 2893, '实心球': 2894, '杜鹃花': 2895, '名山': 2896, '图片': 2897, '河流': 2898, '大人': 2899, '前轮': 2900, '斑马': 2901, '轻': 2902, '灯泡': 2903, '道口': 2904, '算题': 2905, '班主任': 2906, '谁': 2907, '本身': 2908, '杰': 2909, '收银员': 2910, '找': 2911, '地上': 2912, '占地': 2913, '之后': 2914, '两箱': 2915, '晚': 2916, '鸽子': 2917, '笼子': 2918, '王先生': 2919, '邮局': 2920, '汇款': 2921, '汇费': 2922, '添置': 2923, '家具厂': 2924, '长沙': 2925, '下去': 2926, '体育老师': 2927, '筒': 2928, '休息': 2929, '运回': 2930, '花生油': 2931, '空桶': 2932, '钢板': 2933, '盘': 2934, '瓷砖': 2935, '亿元': 2936, '加盐': 2937, '速': 2938, '接上': 2939, '草莓': 2940, '最近': 2941, '负责': 2942, '浇': 2943, '天平': 2944, '砝码': 2945, '平衡': 2946, '杂志': 2947, '寨': 2948, '尺': 2949, '答案': 2950, '通讯员': 2951, '追赶': 2952, '榨油': 2953, '包括': 2954, '如': 2955, '拿到': 2956, '很': 2957, '小猴子': 2958, '锯开': 2959, '碾': 2960, '马虎': 2961, '减法': 2962, '礼堂': 2963, '产品': 2964, '乘火车': 2965, '租': 2966, '一辆车': 2967, '地板': 2968, '丹': 2969, '划船': 2970, '造纸厂': 2971, '熔铸': 2972, '建造': 2973, '整个': 2974, '摞': 2975, '式': 2976, '陈': 2977, '田': 2978, '兵': 2979, '参赛': 2980, '沙': 2981, '玉': 2982, '资料': 2983, '雨天': 2984, '天数': 2985, '分配': 2986, '班级': 2987, '乒乓': 2988, '菠菜': 2989, '白粉笔': 2990, '规模': 2991, '管道': 2992, '羽绒服': 2993, '地区': 2994, '降水量': 2995, '小狗': 2996, '肖': 2997, '王平': 2998, '公交车': 2999, '选中': 3000, '麻雀': 3001, '通道': 3002, '测量': 3003, '深度': 3004, '蓄水': 3005, '童车': 3006, '小球': 3007, '若干个': 3008, '一定': 3009, '金额': 3010, '早操': 3011, '往后': 3012, '群': 3013, '峰': 3014, '收购站': 3015, '收购': 3016, '中心小学': 3017, '一幢': 3018, '教学大楼': 3019, '变化': 3020, '青少年': 3021, '婴儿': 3022, '广告': 3023, '农机厂': 3024, '在校': 3025, '读书': 3026, '次品': 3027, '鸟': 3028, '小鸟': 3029, '论文': 3030, '棒': 3031, '成长': 3032, '稿子': 3033, '饼干': 3034, '一趟': 3035, '公亩': 3036, '服用': 3037, '初': 3038, '更换': 3039, '油菜': 3040, '种植': 3041, '真': 3042, '小鸭子': 3043, '毕业生': 3044, '他俩': 3045, '打印机': 3046, '除去': 3047, '打字': 3048, '小雨': 3049, '我家': 3050, '几辆': 3051, '花生仁': 3052, '纸箱': 3053, '所剩': 3054, '第一队': 3055, '\\ue5e5': 3056, '航程': 3057, '蚂蚁': 3058, '看作': 3059, '几段': 3060, '黄色': 3061, '食盐': 3062, '季度': 3063, '动车': 3064, '副食店': 3065, '迟到': 3066, '棉田': 3067, '国光': 3068, '批发部': 3069, '做成': 3070, '单元': 3071, '养牛': 3072, '看见': 3073, '闪电': 3074, '雷声': 3075, '空中': 3076, '设备': 3077, '爱好': 3078, '直接': 3079, '人员': 3080, '告诉': 3081, '但是': 3082, '向日葵': 3083, '计算结果': 3084, '电': 3085, '辆数': 3086, '张大妈': 3087, '进水管': 3088, '出水管': 3089, 'L': 3090, '打开': 3091, '池水': 3092, '奥': 3093, '总面积': 3094, '宁波': 3095, '规律': 3096, '括号': 3097, '青': 3098, '攒': 3099, '花圃': 3100, '月季': 3101, '匹': 3102, '漏': 3103, '放学': 3104, '装入': 3105, '小袋': 3106, '长江': 3107, '讲': 3108, '色拉油': 3109, '杉树': 3110, '够吃': 3111, '这家': 3112, '食品厂': 3113, '每批': 3114, '会员': 3115, '一栋': 3116, '节日期间': 3117, '鱼塘': 3118, '体育用品': 3119, '书柜': 3120, '期限': 3121, '安排': 3122, '整修': 3123, '环行': 3124, '分配任务': 3125, '键': 3126, '物品': 3127, '搭配': 3128, '改建': 3129, '祝': 3130, '喜': 3131, '车身': 3132, '升高': 3133, '小瓶': 3134, '老板': 3135, '列出': 3136, '下面': 3137, '涨价': 3138, '塑料绳': 3139, '非洲': 3140, '野狗': 3141, '海豚': 3142, '福州': 3143, '服饰': 3144, '甲种': 3145, '乙种': 3146, '肉': 3147, '扎': 3148, '某县': 3149, '取法': 3150, '拼成': 3151, '框架': 3152, '上坡路': 3153, '两侧': 3154, 'VCD': 3155, '土地': 3156, '污染': 3157, '植物': 3158, '废电池': 3159, '医院': 3160, '平时': 3161, '配成': 3162, '含水量': 3163, '雨燕': 3164, '信鸽': 3165, '每星期': 3166, '朝阳': 3167, '乐队': 3168, '全路': 3169, '酒店': 3170, '该厂': 3171, '浇水': 3172, '孩子': 3173, '早餐': 3174, '检查': 3175, '身体': 3176, '所得税': 3177, '受': 3178, '买卖': 3179, '贝': 3180, '乌龟': 3181, '放进': 3182, '豆角': 3183, '中年级': 3184, '低年级': 3185, '高年级': 3186, '木杆': 3187, '织布机': 3188, '部队': 3189, '班长': 3190, '保险金额': 3191, '小型': 3192, '每间': 3193, '日光灯': 3194, '光盘': 3195, '太平洋': 3196, '大西洋': 3197, '钢铁厂': 3198, '水稻': 3199, '客人': 3200, '倍数': 3201, '人行道': 3202, '集邮': 3203, '型号': 3204, '硬座': 3205, '黑白': 3206, '黑子': 3207, '白子': 3208, '资助': 3209, '剧院': 3210, '楼上': 3211, '制服': 3212, '红糖': 3213, '烟台': 3214, '龙': 3215, '穿': 3216, '食品': 3217, '木箱': 3218, '打球': 3219, '游船': 3220, '电风扇': 3221, '我市': 3222, '国道': 3223, '拓宽': 3224, '红纸': 3225, '展览馆': 3226, '游览': 3227, '装进': 3228, '一节': 3229, '讲解': 3230, '高速路': 3231, '销售价': 3232, '锻炼': 3233, '毫克': 3234, '招收': 3235, '级新生': 3236, '生猪': 3237, '东东': 3238, '小马': 3239, '出来': 3240, '长满': 3241, '半个': 3242, '雪梨': 3243, '从小': 3244, '左边': 3245, '右边': 3246, '超': 3247, '兴趣': 3248, '枯井': 3249, '几套': 3250, '放水': 3251, '甜瓜': 3252, '改写': 3253, '爬行': 3254, '圆圈': 3255, '借出去': 3256, '泰州': 3257, '及格率': 3258, '自动扶梯': 3259, '老': 3260, '老工人': 3261, '方便面': 3262, '缸': 3263, '共计': 3264, '世博': 3265, '上班': 3266, '养猪': 3267, '圆锥形': 3268, '均匀': 3269, '滚筒': 3270, '压': 3271, '路面': 3272, '沈阳': 3273, '上课': 3274, '旅行社': 3275, '支付': 3276, '住宿费': 3277, '卡片': 3278, '新购': 3279, '坐在': 3280, '耕地面积': 3281, '新兴': 3282, '铁轨': 3283, '旧': 3284, '旅客': 3285, '灯笼': 3286, '生活': 3287, '救助': 3288, '贫困地区': 3289, '失学': 3290, '温度': 3291, '℃': 3292, '山': 3293, '娟': 3294, '空气': 3295, '传播速度': 3296, '电脑桌': 3297, '牦牛': 3298, '两边': 3299, '表妹': 3300, '舅舅': 3301, '滑雪': 3302, '缆车': 3303, '失去': 3304, '黄豆': 3305, '蛋白质': 3306, '山洞': 3307, '酥糖': 3308, '一端': 3309, '溶液': 3310, '饲料': 3311, '整体': 3312, '轮': 3313, '牧场': 3314, '奶羊': 3315, '龟': 3316, '睡': 3317, '每杯': 3318, '～': 3319, '打篮球': 3320, '宇航员': 3321, '重庆': 3322, '一名': 3323, '收款': 3324, '年前': 3325, '岁数': 3326, '北': 3327, '小苗': 3328, '培育': 3329, '苗': 3330, '柏树': 3331, '热水瓶': 3332, '信封': 3333, '气温': 3334, '大型': 3335, '教科书': 3336, '昨天上午': 3337, '实践': 3338, '近视': 3339, '松鼠': 3340, '松果': 3341, '水龙头': 3342, '浪费': 3343, '两名': 3344, '医疗保险': 3345, '条款': 3346, '医疗费': 3347, '补偿': 3348, '线': 3349, '住院治疗': 3350, '指导': 3351, '代表团': 3352, '整块': 3353, '运行': 3354, '宇宙飞船': 3355, '抄': 3356, '老虎': 3357, '兴': 3358, '博': 3359, '旅游车': 3360, '各行': 3361, '阳': 3362, '热': 3363, '走路': 3364, '纽扣': 3365, '钢条': 3366, '含水率': 3367, '共养': 3368, '配置': 3369, '东北虎': 3370, '欣欣': 3371, '实': 3372, '客船': 3373, '货船': 3374, '沙发': 3375, '新民': 3376, '芹菜': 3377, '李强': 3378, '万辆': 3379, '转动': 3380, '未加工': 3381, '茶杯': 3382, 'l': 3383, '车运': 3384, '器': 3385, '黑色': 3386, '篮子': 3387, '荔枝': 3388, '电动车': 3389, '领': 3390, '仁': 3391, '叶': 3392, '投保': 3393, '投入': 3394, '电脑城': 3395, '经理': 3396, '吧': 3397, '奶粉': 3398, '供': 3399, '法国': 3400, '蜻蜓': 3401, '蜡烛': 3402, '吹灭': 3403, '服': 3404, '送来': 3405, '创': 3406, '菜农': 3407, '供应': 3408, '自然数': 3409, '灌': 3410, '种草': 3411, '短裤': 3412, '百科知识': 3413, '清理': 3414, '\\u3000': 3415, '大队': 3416, '王明': 3417, '每秒钟': 3418, '勇': 3419, '看台': 3420, '门口': 3421, '风衣': 3422, '大圆': 3423, '姥姥家': 3424, '什锦糖': 3425, '卷': 3426, '印刷': 3427, '挂': 3428, '家长': 3429, '捐出': 3430, '敬老院': 3431, '方队': 3432, '派': 3433, '农村': 3434, '赛': 3435, '可用': 3436, '儿童读物': 3437, '一等奖': 3438, '小牛': 3439, '喂': 3440, '制成': 3441, '平均年龄': 3442, '草帽': 3443, '武汉长江大桥': 3444, '玻璃缸': 3445, '阳阳': 3446, '作业本': 3447, '东风': 3448, '乘法': 3449, '层高': 3450, '垂直': 3451, '羚羊': 3452, '秒钟': 3453, '不用': 3454, '冰棒': 3455, '老人': 3456, '知识': 3457, '因此': 3458, '怎样': 3459, '合唱团': 3460, '演出服': 3461, '去掉': 3462, '最低': 3463, '交给': 3464, '高铁': 3465, '印度洋': 3466, '运动场': 3467, '选票': 3468, '当选': 3469, '她们': 3470, '一半多': 3471, '铁': 3472, '公尺': 3473, '车厢': 3474, '小客车': 3475, '平安保险': 3476, '加工厂': 3477, '冬季': 3478, '双休日': 3479, '另一端': 3480, '苹果汁': 3481, '第一层': 3482, '身上': 3483, '窗户': 3484, '趟': 3485, '股票': 3486, '趣味': 3487, '机': 3488, '彩球': 3489, '短绳': 3490, '长绳': 3491, '套房': 3492, '电话费': 3493, '过生日': 3494, '边上': 3495, '几朵': 3496, '打包': 3497, '骑车去': 3498, '大河': 3499, '绣': 3500, '播种面积': 3501, '造价': 3502, '沙石': 3503, '学习机': 3504, '共生': 3505, '帐篷': 3506, '如下': 3507, '办公室': 3508, '刷': 3509, '奶': 3510, '栏目': 3511, '一颗': 3512, '切成': 3513, '棋类': 3514, '先买': 3515, '月饼': 3516, '检修': 3517, '少儿': 3518, '千纸鹤': 3519, '马上': 3520, '模型': 3521, '鲸': 3522, '万部': 3523, '科幻': 3524, '剧场': 3525, '这场': 3526, 'c': 3527, '写作': 3528, '展厅': 3529, '增值税': 3530, '碾米机': 3531, '碾米': 3532, '火车站': 3533, '下坡路': 3534, '斤': 3535, '粮仓': 3536, '饼': 3537, '鱼池': 3538, '香': 3539, '萍': 3540, '券': 3541, '果': 3542, '毛笔字': 3543, '钉': 3544, '会议': 3545, '皮衣': 3546, '院': 3547, '投篮': 3548, '命中率': 3549, '投中': 3550, '牡丹花': 3551, '停留': 3552, '售完': 3553, '蜂鸟': 3554, '窗口': 3555, '童': 3556, '水笔': 3557, '教学': 3558, '桥梁': 3559, '大红花': 3560, '美术班': 3561, '武术': 3562, '一个包': 3563, '晚报': 3564, '旅游团': 3565, '银行贷款': 3566, '浮萍': 3567, '编成': 3568, '起跑线': 3569, '载客': 3570, '每股': 3571, '故障': 3572, '手续费': 3573, '两年': 3574, '听': 3575, '纸鹤': 3576, '柑桔': 3577, '类书': 3578, '锻炼身体': 3579, '身长': 3580, '开垦': 3581, '面上': 3582, '山雀': 3583, '大熊猫': 3584, '温度计': 3585, '摄氏度': 3586, '示': 3587, 's': 3588, '开支': 3589, '昆虫': 3590, '桃园': 3591, '集邮票': 3592, '驯鹿': 3593, '缝': 3594, '棉衣': 3595, '拥有': 3596, '沙包': 3597, '蒙': 3598, '租金': 3599, '原先': 3600, '预算': 3601, '请假': 3602, '晶晶': 3603, '王经理': 3604, '磨面': 3605, '分针': 3606, '家畜': 3607, '螺母': 3608, '抢修': 3609, '通行': 3610, '类': 3611, '鸟类': 3612, '画报': 3613, '陆': 3614, '一路': 3615, '行进': 3616, '象': 3617, '猎犬': 3618, '外公': 3619, '手抄报': 3620, '社区': 3621, '清运': 3622, '总支出': 3623, '造': 3624, '货主': 3625, '两本': 3626, '小雪': 3627, '通话': 3628, '跳舞': 3629, '空调器': 3630, '排空': 3631, '蘑菇': 3632, '风筝': 3633, '气象': 3634, '建成': 3635, '毛线': 3636, '行李': 3637, '球拍': 3638, '字数': 3639, '橡皮': 3640, '灯': 3641, '围棋': 3642, '纸袋': 3643, '概率': 3644, '工作量': 3645, '话费': 3646, '小树': 3647, '器乐': 3648, '俊': 3649, '污水': 3650, '受到': 3651, '抬': 3652, '马桶': 3653, '喜爱': 3654, '食品盒': 3655, '鹏': 3656, '编织': 3657, '树上': 3658, '草原': 3659, '牛顿': 3660, '采煤': 3661, '雨伞': 3662, '半': 3663, '_': 3664, '凯': 3665, '船上': 3666, '田径运动': 3667, '桶装': 3668, '驾车': 3669, '消毒液': 3670, '味精': 3671, '种花': 3672, '铜': 3673, '水田': 3674, '白纸': 3675, '骆驼': 3676, '砌': 3677, '艇': 3678, '优等生': 3679, '房': 3680, '期刊': 3681, '梁': 3682, '录取': 3683, '野兔': 3684, '珠子': 3685, '木材': 3686, '煤炭': 3687, '两包': 3688, '代表': 3689, '舰': 3690, '巍': 3691}\n"
     ]
    }
   ],
   "source": [
    "print(input1_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': 0, 'UNK': 1, 'ns': 2, 'nz': 3, 'n': 4, 'u': 5, 'v': 6, 'm': 7, 'd': 8, 'wp': 9, 'k': 10, 'r': 11, 'ws': 12, 'q': 13, 'nd': 14, 'nt': 15, 'p': 16, 'a': 17, 'nh': 18, 'c': 19, 'ni': 20, 'j': 21, 'b': 22, 'z': 23, 'nl': 24, 'i': 25, 'o': 26, 'e': 27}\n"
     ]
    }
   ],
   "source": [
    "print(input2_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*': 0, '-': 1, '+': 2, '/': 3, '^': 4, '1': 5, '3.14': 6, 'N0': 7, 'N1': 8, 'N2': 9, 'N3': 10, 'N4': 11, 'N5': 12, 'N6': 13, 'N7': 14, 'N8': 15, 'N9': 16, 'N10': 17, 'N11': 18, 'N12': 19, 'N13': 20, 'N14': 21, 'UNK': 22}\n"
     ]
    }
   ],
   "source": [
    "print(output1_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, '*': 1, '+': 2, '/': 3, '^': 4, 'PAD': 5, 'EOS': 6, '1': 7, '3.14': 8, 'N0': 9, 'N1': 10, 'N2': 11, 'N3': 12, 'N4': 13, 'N5': 14, 'N6': 15, 'N7': 16, 'N8': 17, 'N9': 18, 'N10': 19, 'N11': 20, 'N12': 21, 'N13': 22, 'N14': 23, 'SOS': 24, 'UNK': 25}\n"
     ]
    }
   ],
   "source": [
    "print(output2_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example id :  11\n",
      "example input seq :  ['要', '修', '一段', '长', 'NUM', '千米', '的', '路', '，', '第一天', '修', '了', 'NUM', '千米', '，', '第', '二', '天', '修', '了', '余下', '的', 'NUM', '，', '还', '剩下', '多少', '千米', '没有', '修', '完', '？']\n",
      "example question pos(pos指的是词性) :  ['v', 'v', 'm', 'a', 'ws', 'q', 'u', 'n', 'wp', 'nt', 'v', 'u', 'ws', 'q', 'wp', 'm', 'm', 'q', 'v', 'u', 'v', 'u', 'ws', 'wp', 'd', 'v', 'r', 'q', 'd', 'v', 'v', 'wp']\n",
      "example syntatic parser(句法分析) :  [1, -1, 5, 5, 5, 7, 5, 1, 1, 10, 1, 10, 13, 10, 10, 16, 17, 18, 1, 18, 22, 20, 18, 18, 25, 18, 27, 25, 29, 25, 29, 1]\n",
      "example prefix expression :  ['-', '-', 'N0', '*', '-', 'N0', 'N1', 'N2', 'N1']\n",
      "example postfix expression :  ['N0', 'N0', 'N1', '-', 'N2', '*', '-', 'N1', '-']\n",
      "example question nums :  ['24', '4', '(3/8)']\n",
      "example question nums_pos :  [4, 12, 22]\n"
     ]
    }
   ],
   "source": [
    "train_example=pairs_trained[10]\n",
    "print(\"example id : \",train_example[0])\n",
    "print(\"example input seq : \",train_example[1])\n",
    "print(\"example question pos(pos指的是词性) : \",train_example[2])\n",
    "print(\"example syntatic parser(句法分析) : \",train_example[3])\n",
    "print(\"example prefix expression : \",train_example[4])\n",
    "print(\"example postfix expression : \",train_example[5])\n",
    "print(\"example question nums : \",train_example[6])\n",
    "print(\"example question nums_pos : \",train_example[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example id :  19\n",
      "example input seq :  ['小', '芳', '家', 'NUM', '月份', '用水量', '是', 'NUM', '吨', '，', '每吨', '水', '的', '价格', '是', 'NUM', '元', '，', '小', '芳', '家', '一共', '有', 'NUM', '口', '人', '，', '平均', '每人', '应交', '多少', '水费', '？']\n",
      "example question pos(pos指的是词性) :  ['a', 'nh', 'n', 'ws', 'n', 'n', 'v', 'ws', 'q', 'wp', 'r', 'n', 'u', 'n', 'v', 'ws', 'q', 'wp', 'a', 'nh', 'n', 'd', 'v', 'ws', 'q', 'n', 'wp', 'a', 'r', 'v', 'r', 'n', 'wp']\n",
      "example syntatic parser(句法分析) :  [1, 2, 4, 4, 5, 6, -1, 8, 6, 6, 11, 13, 11, 14, 6, 16, 14, 14, 19, 20, 22, 22, 6, 24, 25, 22, 22, 28, 29, 22, 31, 29, 6]\n",
      "example prefix expression :  ['/', '*', 'N1', 'N2', '5']\n",
      "example postfix expression :  ['N1', 'N2', '*', '5', '/']\n",
      "example question nums :  ['5', '16.5', '2.1', '5']\n",
      "example question nums_pos :  [3, 7, 15, 23]\n",
      "[[0, 3]]\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs_trained[:100]:\n",
    "    num_stack = []\n",
    "    for word in pair[4]:\n",
    "        #pair[4]是前缀表达式,行如['-', '-', 'N0', '*', '-', 'N0', 'N1', 'N2', 'N1']\n",
    "        temp_num = []\n",
    "        flag_not = True\n",
    "        #output1_lang是树形结构decoder的词空间\n",
    "        if word not in output1_lang.index2word:\n",
    "            #print(word)\n",
    "            flag_not = False\n",
    "            for i, j in enumerate(pair[6]):\n",
    "                #pair[6]是nums,也就是每一个数字\n",
    "                if j == word:\n",
    "                    temp_num.append(i)\n",
    "\n",
    "        if not flag_not and len(temp_num) != 0:\n",
    "            num_stack.append(temp_num)\n",
    "        if not flag_not and len(temp_num) == 0:\n",
    "            num_stack.append([_ for _ in range(len(pair[6]))])\n",
    "    if num_stack!=[]:\n",
    "        train_example=pair\n",
    "        print(\"example id : \",train_example[0])\n",
    "        print(\"example input seq : \",train_example[1])\n",
    "        print(\"example question pos(pos指的是词性) : \",train_example[2])\n",
    "        print(\"example syntatic parser(句法分析) : \",train_example[3])\n",
    "        print(\"example prefix expression : \",train_example[4])\n",
    "        print(\"example postfix expression : \",train_example[5])\n",
    "        print(\"example question nums : \",train_example[6])\n",
    "        print(\"example question nums_pos : \",train_example[7])\n",
    "        print(num_stack)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence, tree=False):\n",
    "    res = []\n",
    "    for word in sentence:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word in lang.word2index:\n",
    "            res.append(lang.word2index[word])\n",
    "        else:\n",
    "            res.append(lang.word2index[\"UNK\"])\n",
    "    if \"EOS\" in lang.index2word and not tree:\n",
    "        res.append(lang.word2index[\"EOS\"])\n",
    "    return res\n",
    "\n",
    "\n",
    "def texts_from_sentence(lang, sentence, tree=False):\n",
    "    res = []\n",
    "    for word in sentence:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word in lang.word2index:\n",
    "            res.append(word)\n",
    "        else:\n",
    "            res.append(\"UNK\")\n",
    "    if \"EOS\" in lang.index2word and not tree:\n",
    "        res.append(lang.word2index[\"EOS\"])\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_list_processed(num_list):\n",
    "    st = []\n",
    "    for p in num_list:\n",
    "        pos1 = re.search(\"\\d+\\(\", p)\n",
    "        pos2 = re.search(\"\\)\\d+\", p)\n",
    "        if pos1:\n",
    "            st.append(eval(p[pos1.start(): pos1.end() - 1] + \"+\" + p[pos1.end() - 1:]))\n",
    "        elif pos2:\n",
    "            st.append(eval(p[:pos2.start() + 1] + \"+\" + p[pos2.start() + 1: pos2.end()]))\n",
    "        elif p[-1] == \"%\":\n",
    "            st.append(float(p[:-1]) / 100)\n",
    "        else:\n",
    "            st.append(eval(p))\n",
    "    return st\n",
    "\n",
    "\n",
    "def num_order_processed(num_list):\n",
    "    num_order = []\n",
    "    num_array = np.asarray(num_list)\n",
    "    for num in num_array:\n",
    "        num_order.append(sum(num>num_array)+1)\n",
    "    \n",
    "    return num_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19', ['小', '芳', '家', 'NUM', '月份', '用水量', '是', 'NUM', '吨', '，', '每吨', '水', '的', '价格', '是', 'NUM', '元', '，', '小', '芳', '家', '一共', '有', 'NUM', '口', '人', '，', '平均', '每人', '应交', '多少', '水费', '？'], ['a', 'nh', 'n', 'ws', 'n', 'n', 'v', 'ws', 'q', 'wp', 'r', 'n', 'u', 'n', 'v', 'ws', 'q', 'wp', 'a', 'nh', 'n', 'd', 'v', 'ws', 'q', 'n', 'wp', 'a', 'r', 'v', 'r', 'n', 'wp'], [1, 2, 4, 4, 5, 6, -1, 8, 6, 6, 11, 13, 11, 14, 6, 16, 14, 14, 19, 20, 22, 22, 6, 24, 25, 22, 22, 28, 29, 22, 31, 29, 6], ['/', '*', 'N1', 'N2', '5'], ['N1', 'N2', '*', '5', '/'], ['5', '16.5', '2.1', '5'], [3, 7, 15, 23]]\n"
     ]
    }
   ],
   "source": [
    "print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_cell = indexes_from_sentence(input1_lang, pair[1])#pair[1] is input_seq\n",
    "texts_cell = texts_from_sentence(input1_lang, pair[1])\n",
    "input2_cell = indexes_from_sentence(input2_lang, pair[2])#pair[2] is input seq pos\n",
    "output1_cell = indexes_from_sentence(output1_lang, pair[4], True)#pair[4] is prefix_expression, used for tree-decoder\n",
    "output2_cell = indexes_from_sentence(output2_lang, pair[5], False)#pair[5] is postfix expression, \n",
    "num_list = num_list_processed(pair[6])#pair[6] is nums\n",
    "num_order = num_order_processed(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 16.5, 2.1, 5]\n"
     ]
    }
   ],
   "source": [
    "print(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['小', '芳', '家', 'NUM', '月份', 'UNK', '是', 'NUM', '吨', '，', '每吨', '水', '的', '价格', '是', 'NUM', '元', '，', '小', '芳', '家', '一共', '有', 'NUM', '口', '人', '，', '平均', '每人', '应交', '多少', '水费', '？']\n"
     ]
    }
   ],
   "source": [
    "print(texts_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 4, 12, 4, 4, 6, 12, 13, 9, 11, 4, 5, 4, 6, 12, 13, 9, 17, 18, 4, 8, 6, 12, 13, 4, 9, 17, 11, 6, 11, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "print(input2_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 8, 9, 22]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 1, 25, 3, 6]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*': 0, '-': 1, '+': 2, '/': 3, '^': 4, '1': 5, '3.14': 6, 'N0': 7, 'N1': 8, 'N2': 9, 'N3': 10, 'N4': 11, 'N5': 12, 'N6': 13, 'N7': 14, 'N8': 15, 'N9': 16, 'N10': 17, 'N11': 18, 'N12': 19, 'N13': 20, 'N14': 21, 'UNK': 22}\n"
     ]
    }
   ],
   "source": [
    "print(output1_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, '*': 1, '+': 2, '/': 3, '^': 4, 'PAD': 5, 'EOS': 6, '1': 7, '3.14': 8, 'N0': 9, 'N1': 10, 'N2': 11, 'N3': 12, 'N4': 13, 'N5': 14, 'N6': 15, 'N7': 16, 'N8': 17, 'N9': 18, 'N10': 19, 'N11': 20, 'N12': 21, 'N13': 22, 'N14': 23, 'SOS': 24, 'UNK': 25}\n"
     ]
    }
   ],
   "source": [
    "print(output2_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 16.5, 2.1, 5]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_list_processed(num_list):\n",
    "    st = []\n",
    "    for p in num_list:\n",
    "        pos1 = re.search(\"\\d+\\(\", p)\n",
    "        pos2 = re.search(\"\\)\\d+\", p)\n",
    "        if pos1:\n",
    "            st.append(eval(p[pos1.start(): pos1.end() - 1] + \"+\" + p[pos1.end() - 1:]))\n",
    "        elif pos2:\n",
    "            st.append(eval(p[:pos2.start() + 1] + \"+\" + p[pos2.start() + 1: pos2.end()]))\n",
    "        elif p[-1] == \"%\":\n",
    "            st.append(float(p[:-1]) / 100)\n",
    "        else:\n",
    "            st.append(eval(p))\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.25, 3.875]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list_processed(['20%','(2/8)','3(7/8)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_order_processed(num_list):\n",
    "    num_order = []\n",
    "    num_array = np.asarray(num_list)\n",
    "    for num in num_array:\n",
    "        num_order.append(sum(num>num_array)+1)\n",
    "    \n",
    "    return num_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '16.5', '2.1', '5']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 3]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_order_processed(pair[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 16.5, 2.1, 5]\n"
     ]
    }
   ],
   "source": [
    "num_list = num_list_processed(pair[6])#pair[6] is nums\n",
    "print(num_list)\n",
    "num_order = num_order_processed(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 2]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang,sentence,tree=False):\n",
    "    '''\n",
    "    根据lang中的word2index将sentence中的每一个token转为对应的id\n",
    "    这里面的sentence不一定是句子，也可能是词性标注序列，或者输出的前缀后缀表达式\n",
    "    '''\n",
    "    res=[]\n",
    "    unk_token=lang.word2index['UNK']\n",
    "    for token in sentence:\n",
    "        if len(token)==0:\n",
    "            continue\n",
    "        res.append(lang.word2index.get(token,unk_token))\n",
    "    if 'EOS' in lang.index2word and not tree:\n",
    "        #输出端有两个decoder，其中一个是sequence式结构，另一个是tree结构\n",
    "        #sequence结构中需要有'EOS'\n",
    "        res.append(lang.word2index['EOS'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example id :  501\n",
      "example input seq :  ['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', '童鞋', '的', '价格下降', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', '童鞋', '的', '原价', '=', '多少', '元', '？']\n",
      "example question pos(pos指的是词性) :  ['nz', 'n', 'v', 'wp', 'v', 'ws', 'm', 'wp', 'v', 'v', 'wp', 'm', 'n', 'u', 'n', 'ws', 'nd', 'd', 'v', 'ws', 'q', 'wp', 'r', 'u', 'n', 'v', 'ws', 'q', 'wp', 'r', 'n', 'u', 'n', 'wp', 'r', 'q', 'wp']\n",
      "example syntatic parser(句法分析) :  [1, 2, -1, 4, 9, 4, 4, 4, 9, 2, 2, 12, 14, 12, 15, 16, 18, 18, 2, 20, 18, 18, 24, 22, 25, 18, 27, 25, 18, 30, 32, 30, 35, 35, 35, 18, 2]\n",
      "example prefix expression :  ['/', '+', 'N3', 'N2', '-', '1', 'N1']\n",
      "example postfix expression :  ['N3', 'N2', '+', '1', 'N1', '-', '/']\n",
      "example question nums :  ['6', '20%', '8', '56']\n",
      "example question nums_pos :  [5, 15, 19, 26]\n"
     ]
    }
   ],
   "source": [
    "train_example=pairs_trained[500]\n",
    "print(\"example id : \",train_example[0])\n",
    "print(\"example input seq : \",train_example[1])\n",
    "print(\"example question pos(pos指的是词性) : \",train_example[2])\n",
    "print(\"example syntatic parser(句法分析) : \",train_example[3])\n",
    "print(\"example prefix expression : \",train_example[4])\n",
    "print(\"example postfix expression : \",train_example[5])\n",
    "print(\"example question nums : \",train_example[6])\n",
    "print(\"example question nums_pos : \",train_example[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1443, 1444, 286, 691, 1445, 1, 35, 693, 1249, 604, 26, 515, 2, 6, 2, 1, 117, 468, 1446, 1, 190, 26, 64, 6, 189, 61, 1, 190, 13, 347, 2, 6, 371, 89, 34, 190, 52]\n",
      "[3, 4, 6, 9, 6, 12, 7, 9, 6, 6, 9, 7, 4, 5, 4, 12, 14, 8, 6, 12, 13, 9, 11, 5, 4, 6, 12, 13, 9, 11, 4, 5, 4, 9, 11, 13, 9]\n",
      "[12, 11, 2, 7, 10, 0, 3, 6]\n",
      "[3, 2, 10, 9, 1, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "input_seq_ids=indexes_from_sentence(input1_lang,sentence=train_example[1])\n",
    "print(input_seq_ids)\n",
    "input_seq_pos_ids=indexes_from_sentence(input2_lang,sentence=train_example[2])\n",
    "print(input_seq_pos_ids)\n",
    "#output2_lang对应的是sequence结构的decoder词汇\n",
    "#train_example[5]是后缀\n",
    "output_sequence_ids=indexes_from_sentence(output2_lang,sentence=train_example[5],tree=False)\n",
    "output_tree_ids=indexes_from_sentence(output1_lang,sentence=train_example[4],tree=True)#train_example[4]是前缀\n",
    "print(output_sequence_ids)\n",
    "print(output_tree_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_from_sentence(lang, sentence, tree=False):\n",
    "    '''\n",
    "    函数的目的是将sentence中出现的词汇如果不在lang.word2index中，那么就换成UNK\n",
    "    '''\n",
    "    res = []\n",
    "    for word in sentence:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word in lang.word2index:\n",
    "            res.append(word)\n",
    "        else:\n",
    "            res.append(\"UNK\")\n",
    "    if \"EOS\" in lang.index2word and not tree:\n",
    "        res.append(lang.word2index[\"EOS\"])\n",
    "    return res\n",
    "\n",
    "def num_list_processed(num_list):\n",
    "    '''\n",
    "    num_list代表的是一个问题中所有的数字\n",
    "    函数的目的是将num_list中的数字进一步换算成对应的值，同时将百分号等数字替换成对应的小数\n",
    "    将分数也同样计算成对应的小数\n",
    "    '''\n",
    "    st = []\n",
    "    for p in num_list:\n",
    "        pos1 = re.search(\"\\d+\\(\", p)\n",
    "        pos2 = re.search(\"\\)\\d+\", p)\n",
    "        if pos1:\n",
    "            st.append(eval(p[pos1.start(): pos1.end() - 1] + \"+\" + p[pos1.end() - 1:]))\n",
    "        elif pos2:\n",
    "            st.append(eval(p[:pos2.start() + 1] + \"+\" + p[pos2.start() + 1: pos2.end()]))\n",
    "        elif p[-1] == \"%\":\n",
    "            st.append(float(p[:-1]) / 100)\n",
    "        else:\n",
    "            st.append(eval(p))\n",
    "    return st\n",
    "\n",
    "def num_order_processed(num_list):\n",
    "    '''\n",
    "    由于论文中提出要比较一个问题中所有数字的大小，所以这个函数的作用就是用整数来表达一个数字在当前这个问题中的所有\n",
    "    数字的大小，数值的大小代表的是这个数字大于多少个数字\n",
    "    '''\n",
    "    num_order = []\n",
    "    num_array = np.asarray(num_list)\n",
    "    for num in num_array:\n",
    "        num_order.append(sum(num>num_array)+1)\n",
    "    \n",
    "    return num_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', '童鞋', '的', '价格下降', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', '童鞋', '的', '原价', '=', '多少', '元', '？']\n",
      "['6', '20%', '8', '56']\n",
      "-------------------------------------------------------------------\n",
      "['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', 'UNK', '的', 'UNK', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', 'UNK', '的', '原价', '=', '多少', '元', '？']\n",
      "[6, 0.2, 8, 56]\n",
      "[2, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "sentence=train_example[1]\n",
    "num_list=train_example[6]\n",
    "print(sentence)\n",
    "print(num_list)\n",
    "print('-------------------------------------------------------------------')\n",
    "print(texts_from_sentence(input1_lang,sentence))\n",
    "num_list=num_list_processed(num_list)\n",
    "print(num_list)\n",
    "print(num_order_processed(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2表示的是，在nums中的所有数字中，6这个数字大于其中的两个数字（包括自己）\n",
    "# 1表示的是，在nums中的所有数字中，0.2这个数字大于其中的一个数字（包括自己）\n",
    "# 3表示的是，在nums中的所有数字中，8这个数字大于其中的三个数字（包括自己）\n",
    "# 4表示的是，在nums中的所有数字中，56这个数字大于其中的四个数字（包括自己）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(pairs_trained,pairs_tested,trim_min_count,generate_nums,copy_nums):\n",
    "    '''\n",
    "    pairs[0]-->id，问题样本id\n",
    "    pairs[1]-->input seq，问题文本\n",
    "    pairs[2]-->pos，问题单词的词性标注\n",
    "    pairs[3]-->parser,句法分析的结果\n",
    "    pairs[4]-->prefix expression\n",
    "    pairs[5]-->postfix expression\n",
    "    pairs[6]-->nums\n",
    "    pairs[7]-->nums_pos\n",
    "    '''\n",
    "    input1_lang = Lang()\n",
    "    input2_lang = Lang()\n",
    "    output1_lang = Lang()\n",
    "    output2_lang = Lang()\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs_trained:\n",
    "        if pair[-1]:\n",
    "            input1_lang.add_sen_to_vocab(pair[1])\n",
    "            input2_lang.add_sen_to_vocab(pair[2])\n",
    "            output1_lang.add_sen_to_vocab(pair[4])\n",
    "            output2_lang.add_sen_to_vocab(pair[5])\n",
    "    \n",
    "    input1_lang.build_input_lang(trim_min_count)\n",
    "    input2_lang.build_input_lang_for_pos()\n",
    "    output1_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "    output2_lang.build_output_lang(generate_nums, copy_nums)\n",
    "\n",
    "    for pair in pairs_trained:\n",
    "        num_stack = []\n",
    "        for word in pair[4]:\n",
    "            #pair[4]是前缀表达式,行如['/', '*', 'N1', 'N2', '5']\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            #output1_lang是树形结构decoder的词空间\n",
    "            if word not in output1_lang.index2word:\n",
    "                #这种情况是因为前缀表达式中出现了数字，而我们知道，数字是不作为词空间中的元素的\n",
    "                #表达式中按理说所有的数字都已经被转为对应的Ni了，出现数字的原因是这个数字在问题中出现了多次\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[6]):\n",
    "                    #pair[6]是nums,也就是每一个数字,行如 ['5', '16.5', '2.1', '5']\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)#temp==[0,3]，temp记录的是表达式中出现的重复的数字在nums中的位置\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[6]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input1_cell = indexes_from_sentence(input1_lang, pair[1])#pair[1] is input_seq\n",
    "        texts_cell = texts_from_sentence(input1_lang, pair[1])\n",
    "        input2_cell = indexes_from_sentence(input2_lang, pair[2])#pair[2] is input seq pos\n",
    "        output1_cell = indexes_from_sentence(output1_lang, pair[4], True)#pair[4] is prefix_expression, used for tree-decoder\n",
    "        output2_cell = indexes_from_sentence(output2_lang, pair[5], False)#pair[5] is postfix expression, \n",
    "        num_list = num_list_processed(pair[6])#pair[6] is nums\n",
    "        num_order = num_order_processed(num_list)\n",
    "        train_pairs.append((pair[0], texts_cell, input1_cell, input2_cell, pair[3], len(input1_cell), \n",
    "                            output1_cell, len(output1_cell), output2_cell, len(output2_cell), \n",
    "                            pair[6], pair[7], num_stack, num_order))\n",
    "    print('Indexed %d words in input language, %d words in output1, %d words in output2' % \n",
    "          (input1_lang.n_words, output1_lang.n_words, output2_lang.n_words))\n",
    "    print('Number of training data %d' % (len(train_pairs)))\n",
    "    for pair in pairs_tested:\n",
    "        num_stack = []\n",
    "        for word in pair[4]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output1_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[6]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[6]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input1_cell = indexes_from_sentence(input1_lang, pair[1])\n",
    "        texts_cell = texts_from_sentence(input1_lang, pair[1])\n",
    "        input2_cell = indexes_from_sentence(input2_lang, pair[2])\n",
    "        output1_cell = indexes_from_sentence(output1_lang, pair[4], True)\n",
    "        output2_cell = indexes_from_sentence(output2_lang, pair[5], False)\n",
    "        num_list = num_list_processed(pair[6])\n",
    "        num_order = num_order_processed(num_list)\n",
    "        test_pairs.append((pair[0], texts_cell, input1_cell, input2_cell, pair[3], len(input1_cell), \n",
    "                           output1_cell, len(output1_cell), output2_cell, len(output2_cell), \n",
    "                           pair[6], pair[7], num_stack, num_order))\n",
    "    print('Number of testind data %d' % (len(test_pairs)))\n",
    "    return input1_lang, input2_lang, output1_lang, output2_lang, train_pairs, test_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing words...\n",
      "Indexed 3689 words in input language, 23 words in output1, 26 words in output2\n",
      "Number of training data 18528\n",
      "Number of testind data 4634\n"
     ]
    }
   ],
   "source": [
    "input1_lang, input2_lang, output1_lang, output2_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5, generate_nums, copy_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18528"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('501', ['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', 'UNK', '的', 'UNK', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', 'UNK', '的', '原价', '=', '多少', '元', '？'], [1443, 1444, 286, 691, 1445, 1, 35, 693, 1249, 604, 26, 515, 2, 6, 2, 1, 117, 468, 1446, 1, 190, 26, 64, 6, 189, 61, 1, 190, 13, 347, 2, 6, 371, 89, 34, 190, 52], [3, 4, 6, 9, 6, 12, 7, 9, 6, 6, 9, 7, 4, 5, 4, 12, 14, 8, 6, 12, 13, 9, 11, 5, 4, 6, 12, 13, 9, 11, 4, 5, 4, 9, 11, 13, 9], [1, 2, -1, 4, 9, 4, 4, 4, 9, 2, 2, 12, 14, 12, 15, 16, 18, 18, 2, 20, 18, 18, 24, 22, 25, 18, 27, 25, 18, 30, 32, 30, 35, 35, 35, 18, 2], 37, [3, 2, 10, 9, 1, 5, 8], 7, [12, 11, 2, 7, 10, 0, 3, 6], 8, ['6', '20%', '8', '56'], [5, 15, 19, 26], [], [2, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example id :  501\n",
      "example input seq (词频少的单词已经被替换成UNK):  ['新世纪', '百货', '开展', '“', '庆', 'NUM', '一', '”', '促销', '活动', '，', '一种', 'UNK', '的', 'UNK', 'NUM', '后', '再', '降', 'NUM', '元', '，', '这时', '的', '价格', '是', 'NUM', '元', '．', '这种', 'UNK', '的', '原价', '=', '多少', '元', '？']\n",
      "将所有的单词替换成对应的id :  [1443, 1444, 286, 691, 1445, 1, 35, 693, 1249, 604, 26, 515, 2, 6, 2, 1, 117, 468, 1446, 1, 190, 26, 64, 6, 189, 61, 1, 190, 13, 347, 2, 6, 371, 89, 34, 190, 52]\n",
      "将标注的词性替换成对应的id :  [3, 4, 6, 9, 6, 12, 7, 9, 6, 6, 9, 7, 4, 5, 4, 12, 14, 8, 6, 12, 13, 9, 11, 5, 4, 6, 12, 13, 9, 11, 4, 5, 4, 9, 11, 13, 9]\n",
      "句法分析的结构:  [1, 2, -1, 4, 9, 4, 4, 4, 9, 2, 2, 12, 14, 12, 15, 16, 18, 18, 2, 20, 18, 18, 24, 22, 25, 18, 27, 25, 18, 30, 32, 30, 35, 35, 35, 18, 2]\n",
      "句子长度 :  37\n",
      "将前缀表达式中的运算符替换成对应的id :  [3, 2, 10, 9, 1, 5, 8]\n",
      "前缀表达式的长度 :  7\n",
      "将后缀表达式中的运算符替换成对应的id :  [12, 11, 2, 7, 10, 0, 3, 6]\n",
      "后缀表达式的长度(后缀表达式是作为sequence decoder的标签，所以包含EOS，长度要更长一些) :  8\n",
      "这个问题对应的所有的数字 :  ['6', '20%', '8', '56']\n",
      "这个问题中数字的位置 :  [5, 15, 19, 26]\n",
      "这个问题是否包含有重复数字，如果有，重复数字出现的位置 :  []\n",
      "这个问题中所有数字的大小关系 :  [2, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "train_example=train_pairs[500]\n",
    "print(\"example id : \",train_example[0])\n",
    "print(\"example input seq (词频少的单词已经被替换成UNK): \",train_example[1])\n",
    "print(\"将所有的单词替换成对应的id : \",train_example[2])\n",
    "print(\"将标注的词性替换成对应的id : \",train_example[3])\n",
    "print(\"句法分析的结构: \",train_example[4])\n",
    "print(\"句子长度 : \",train_example[5])\n",
    "print(\"将前缀表达式中的运算符替换成对应的id : \",train_example[6])\n",
    "print(\"前缀表达式的长度 : \",train_example[7])\n",
    "print(\"将后缀表达式中的运算符替换成对应的id : \",train_example[8])\n",
    "print(\"后缀表达式的长度(后缀表达式是作为sequence decoder的标签，所以包含EOS，长度要更长一些) : \",train_example[9])\n",
    "print('这个问题对应的所有的数字 : ',train_example[10])\n",
    "print('这个问题中数字的位置 : ',train_example[11])\n",
    "print('这个问题是否包含有重复数字，如果有，重复数字出现的位置 : ',train_example[12])\n",
    "print('这个问题中所有数字的大小关系 : ',train_example[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(train_pairs, embedding_size, input_lang):\n",
    "    sentences = []\n",
    "    for train in train_pairs:\n",
    "        sentence = train[1]\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    from gensim.models import word2vec\n",
    "    model = word2vec.Word2Vec(sentences, size=embedding_size, min_count=1)\n",
    "\n",
    "    emb_vectors = []\n",
    "    emb_vectors.append(np.zeros((embedding_size)))\n",
    "    for i in range(1, input_lang.n_words):\n",
    "        emb_vectors.append(np.array(model.wv[input_lang.index2word[i]]))\n",
    "    \n",
    "    return emb_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "emb_vectors = word2vec(train_pairs, embedding_size, input1_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3689"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3689, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emb_vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAD'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1_lang.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model=nn.Embedding(num_embeddings=len(input1_lang.word2index),embedding_dim=5,padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.5422,  0.1766, -0.9550, -0.3279,  1.0024],\n",
       "        [-0.0716, -0.1338, -1.4883, -1.1652,  1.4293],\n",
       "        ...,\n",
       "        [-0.8252, -0.4496,  3.3214,  0.7274,  2.1875],\n",
       "        [-1.5863,  1.4680,  0.8336, -1.3941,  0.2827],\n",
       "        [-0.1123, -0.8680,  0.5685,  0.1322, -0.8413]], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18528"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_batch(pairs_to_batch,batch_size):\n",
    "    '''\n",
    "    这个函数用来构造输入数据\n",
    "    对于pairs_to_batch中的每一个元素example，都有14个字段，分别是\n",
    "    example id;example input seq (词频少的单词已经被替换成UNK);\n",
    "    example input_seq_id(所有的单词替换成对应的id);example pos_id(将标注的词性替换成对应的id);\n",
    "    example parse(句法分析的结构);example_length(句子长度);\n",
    "    example prefix_expression_id(将前缀表达式中的运算符替换成对应的id);prefix_expression length(前缀表达式的长度);\n",
    "    example postfix_expression_id(将后缀表达式中的运算符替换成对应的id);postfix_expression length(后缀表达式的长度);\n",
    "    example question nums(这个问题对应的所有的数字);example question nums_pos(这个问题中数字的位置);\n",
    "    example question num_stack(这个问题是否包含有重复数字，如果有，重复数字出现的位置);example question num_order(这个问题中所有数字的大小关系)\n",
    "    每一个example有14个字段\n",
    "    '''\n",
    "    \n",
    "    pairs=deepcopy(pairs_to_batch)\n",
    "    random.shuffle(pairs)#随机打乱训练数据，因为我们要保证各个数据样本之间是相互独立的，满足iid条件\n",
    "    \n",
    "    id_batches=[]#存储各个样本的id\n",
    "    input1_batches=[]#存储各个样本中问题对应的id(将问题文本中的单词转成id)\n",
    "    input2_batches=[]#存储各个样本中问题的每一个单词对应的词性标注对应的id\n",
    "    #input1和input2都是sequence encoder的输入\n",
    "    input_lengths=[]#存储各个样本中问题的长度\n",
    "    output1_lengths=[]#存储各个样本中问题对应的前缀表达式的长度\n",
    "    output2_lengths=[]#存储各个样本中问题对应的后缀表达式的长度\n",
    "    nums_batches=[]#存储各个样本中问题中出现的数字个数，也就是len(nums)\n",
    "    num_pos_batches=[]#对应的，存储各个样本中问题中出现的数字在问题中的索引\n",
    "    num_order_batches=[]#存储每一个问题中各个数字之间的大小关系\n",
    "    num_stack_batches=[]#如果问题中出现了重复数字，记录重复数字在nums中的位置，否则是[]\n",
    "    num_size_batches=[]\n",
    "    output1_batches = []\n",
    "    output2_batches = []\n",
    "    parse_graph_batches = []#存储句法解析\n",
    "    \n",
    "    batches=[]#按照批次来存储数据，每一批数据为一个单词\n",
    "    num_of_batch=0\n",
    "    print()\n",
    "    print('一共有{}个训练数据样本，按照{}为批次大小，所以一共有{}个训练批次'.format(len(pairs),batch_size,len(pairs)//batch_size+1))\n",
    "    while num_of_batch+batch_size<len(pairs):\n",
    "        batches.append(pairs[num_of_batch:num_of_batch+batch_size])\n",
    "        num_of_batch+=batch_size\n",
    "    batches.append(pairs[num_of_batch:])\n",
    "    \n",
    "    for batch in batches:\n",
    "        #在每一个批次中，按照这个批次的每一个句子的长度排序，句子长的放在前面，这样有助于后面的RNN编码\n",
    "        batch=sorted(batch,key=lambda example:example[5],reverse=True)#example[5]是句子长度\n",
    "        input_length=[]\n",
    "        output1_length=[]\n",
    "        output2_length=[]\n",
    "        for id_,input_seq,seq_id,pos_id,parse,seq_len,prefix_id,prefix_len,postfix_id,postfix_len,nums,nums_pos,num_stack,num_order in batch:\n",
    "            input_length.append(seq_len)\n",
    "            output1_length.append(prefix_len)\n",
    "            output2_length.append(postfix_len)\n",
    "        input_lengths.append(input_length)\n",
    "        output1_lengths.append(output1_length)\n",
    "        output2_lengths.append(output2_length)\n",
    "        input_len_max = input_length[0]#当前这个批次中所有问题长度的最大值\n",
    "        output1_len_max = max(output1_length)\n",
    "        output2_len_max = max(output2_length)\n",
    "        \n",
    "        id_batch = []\n",
    "        input1_batch = []\n",
    "        input2_batch = []\n",
    "        output1_batch = []\n",
    "        output2_batch = []\n",
    "        num_batch = []\n",
    "        num_stack_batch = []\n",
    "        num_pos_batch = []\n",
    "        num_order_batch = []\n",
    "        num_size_batch = []\n",
    "        parse_tree_batch = []\n",
    "        \n",
    "        for idx,input_seq,seq_id,pos_id,parse,seq_len,prefix_id,prefix_len,postfix_id,postfix_len,num,num_pos,num_stack,num_order in batch:\n",
    "            id_batch.append(idx)\n",
    "            seq_id+=[PAD_token for _ in range(input_len_max-seq_len)]#pad\n",
    "            pos_id+=[PAD_token for _ in range(input_len_max-seq_len)]#pad\n",
    "            input1_batch.append(seq_id)\n",
    "            input2_batch.append(pos_id)\n",
    "            prefix_id+=[PAD_token for _ in range(output1_len_max-prefix_len)]\n",
    "            postfix_id+=[PAD_token for _ in range(output2_len_max-postfix_len)]\n",
    "            #表达式同样需要pad\n",
    "            output1_batch.append(prefix_id)\n",
    "            output2_batch.append(postfix_id)\n",
    "            num_batch.append(len(num))#这个问题出现了多少个数字\n",
    "            num_stack_batch.append(num_stack)#是否有重复数字\n",
    "            num_pos_batch.append(num_pos)#数字的位置\n",
    "            num_order_batch.append(num_order)#数字之间的大小关系\n",
    "            num_size_batch.append(len(num_pos))\n",
    "            assert len(num)==len(num_pos)\n",
    "            parse_tree_batch.append(parse)\n",
    "            \n",
    "        id_batches.append(id_batch)\n",
    "        input1_batches.append(input1_batch)\n",
    "        input2_batches.append(input2_batch)\n",
    "        output1_batches.append(output1_batch)\n",
    "        output2_batches.append(output2_batch)\n",
    "        nums_batches.append(num_batch)\n",
    "        num_stack_batches.append(num_stack_batch)\n",
    "        num_pos_batches.append(num_pos_batch)\n",
    "        num_order_batches.append(num_order_batch)\n",
    "        num_size_batches.append(num_size_batch)\n",
    "        \n",
    "        parse_g=get_parse_graph_batch(input_length, parse_tree_batch)\n",
    "        assert type(parse_g)==np.ndarray\n",
    "        assert parse_g.shape==(len(batch),3,input_len_max,input_len_max)\n",
    "        parse_graph_batches.append(parse_g)\n",
    "        \n",
    "    return id_batches, input1_batches, input2_batches, input_lengths, output1_batches, output1_lengths, output2_batches, output2_lengths, \\\n",
    "       nums_batches, num_stack_batches, num_pos_batches, num_order_batches, num_size_batches, parse_graph_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_graph_batch(input_length, parse_tree_batch):\n",
    "    batch_graph = []\n",
    "    max_len = max(input_length)\n",
    "    for i in range(len(input_length)):\n",
    "        parse_tree = parse_tree_batch[i]\n",
    "        diag_ele = [1] * input_length[i] + [0] * (max_len - input_length[i])\n",
    "        graph1 = np.diag([1]*max_len) + np.diag(diag_ele[1:], 1) + np.diag(diag_ele[1:], -1)\n",
    "        graph2 = copy.deepcopy(graph1)\n",
    "        graph3 = copy.deepcopy(graph1)\n",
    "        for j in range(len(parse_tree)):\n",
    "            if parse_tree[j] != -1:\n",
    "                graph1[j, parse_tree[j]] = 1\n",
    "                graph2[parse_tree[j], j] = 1\n",
    "                graph3[j, parse_tree[j]] = 1\n",
    "                graph3[parse_tree[j], j] = 1\n",
    "        graph = [graph1.tolist(), graph2.tolist(), graph3.tolist()]\n",
    "        batch_graph.append(graph)\n",
    "    batch_graph = np.array(batch_graph)\n",
    "    return batch_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "一共有18528个训练数据样本，按照3为批次大小，所以一共有6177个训练批次\n"
     ]
    }
   ],
   "source": [
    "batch_size=3\n",
    "id_batches, input1_batches, input2_batches, input_lengths, output1_batches, output1_lengths, output2_batches, output2_lengths, \\\n",
    "        nums_batches, num_stack_batches, num_pos_batches, num_order_batches, num_size_batches, parse_graph_batches = prepare_train_batch(train_pairs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['537', '16790', '9453']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "id_batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5483', ['我国', 'UNK', 'UNK', '长', '约', 'NUM', 'km', '，', '是', 'UNK', 'UNK', '长', '的', 'NUM', '；', 'UNK', 'UNK', '长', '是', '陆地', 'UNK', '长', '的', 'NUM', '．', '我国', '陆地', 'UNK', '长', '多少', 'km', '？'], [1052, 2, 2, 33, 447, 1, 1010, 26, 61, 2, 2, 33, 6, 1, 314, 2, 2, 33, 61, 641, 2, 33, 6, 1, 13, 1052, 641, 2, 33, 34, 1010, 52], [4, 4, 4, 17, 8, 12, 12, 9, 6, 4, 4, 17, 5, 12, 9, 4, 4, 17, 6, 4, 4, 17, 5, 12, 9, 4, 4, 4, 6, 11, 12, 9], [1, 2, 6, 6, 5, 6, 8, 6, -1, 10, 11, 13, 11, 8, 8, 16, 17, 18, 8, 20, 21, 23, 21, 18, 18, 27, 27, 28, 18, 30, 28, 8], 32, [3, 3, 7, 8, 9], 5, [9, 10, 3, 11, 3, 6], 6, ['14000', '(7/9)', '(9/11)'], [5, 13, 23], [], [3, 1, 2])\n",
      "('5296', ['UNK', '家', '有', 'UNK', 'NUM', '棵', '，', '平均', '每棵', '可以', '产', 'UNK', 'NUM', '千克', '，', '今年', '每千克', 'UNK', '可以', '卖', 'NUM', '元', '，', '可以', '收入', '多少', '元', '？'], [2, 185, 131, 2, 1, 30, 26, 192, 1879, 78, 1924, 2, 1, 86, 26, 566, 318, 2, 78, 316, 1, 190, 26, 78, 1064, 34, 190, 52], [18, 4, 6, 4, 12, 13, 9, 17, 11, 6, 6, 4, 12, 13, 9, 15, 11, 4, 6, 6, 12, 13, 9, 6, 6, 11, 13, 9], [1, 2, -1, 5, 5, 2, 2, 8, 10, 10, 2, 13, 13, 10, 10, 19, 17, 19, 19, 2, 21, 19, 19, 24, 19, 26, 24, 2], 28, [0, 9, 0, 8, 7], 5, [11, 10, 9, 1, 1, 6], 6, ['80', '12', '9'], [4, 12, 20], [], [3, 2, 1])\n",
      "('13836', ['一', '个数', '的', 'NUM', '比', '它', '的', 'NUM', '多', 'NUM', '，', '求', '这个', '数', '．'], [35, 196, 6, 1, 105, 339, 6, 1, 156, 1, 26, 614, 49, 139, 13], [7, 4, 5, 12, 16, 11, 5, 12, 17, 12, 9, 6, 11, 4, 9], [1, 3, 1, 11, 11, 9, 5, 9, 9, 4, 4, -1, 13, 11, 11], 15, [3, 9, 1, 7, 8], 5, [11, 9, 10, 0, 3, 6], 6, ['(2/5)', '30%', '210'], [3, 7, 9], [], [2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[5482])\n",
    "print(train_pairs[5295])\n",
    "print(train_pairs[13835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_batch=input1_batches[i]\n",
    "input2_batch=input2_batches[i]\n",
    "input_length=input_lengths[i]\n",
    "target1_batch=output1_batches[i]\n",
    "target1_length=output1_lengths[i]\n",
    "target2_batch=output2_batches[i]\n",
    "target2_length=output2_lengths[i] \n",
    "num_stack_batch=num_stack_batches[i]\n",
    "num_size_batch=num_size_batches[i]\n",
    "num_pos_batch=num_pos_batches[i]\n",
    "num_order_batch=num_order_batches[i]\n",
    "parse_graph_batch=parse_graph_batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_num1_ids = []\n",
    "generate_num2_ids = []\n",
    "for num in generate_nums:\n",
    "    generate_num1_ids.append(output1_lang.word2index[num])\n",
    "    generate_num2_ids.append(output2_lang.word2index[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_num1_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_num2_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       dtype=torch.uint8)\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "tensor([[ 7, 18, 27, 35],\n",
      "        [12, 18, -1, -1],\n",
      "        [ 3,  9, 18, -1]])\n",
      "tensor([[4, 2, 1, 3],\n",
      "        [1, 2, 0, 0],\n",
      "        [3, 2, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "seq_mask=[]#构造sequence mask，因为在做注意力时，要避免关注到pad位置的单词\n",
    "max_len=max(input_length)#这两个问题中最长的\n",
    "for seq_len in input_length:\n",
    "    seq_mask.append([0 for _ in range(seq_len)]+[1 for _ in range(max_len-seq_len)])\n",
    "    #做注意时，将seq_mask乘以-1e12然后element_wise add到向量上，然后softmax，pad位置的权重就接近于0\n",
    "seq_mask=torch.ByteTensor(seq_mask)\n",
    "print(seq_mask)\n",
    "num_mask = []\n",
    "max_num_size = max(num_size_batch) + len(generate_num1_ids)\n",
    "for i in num_size_batch:\n",
    "    d = i + len(generate_num1_ids)\n",
    "    num_mask.append([0] * d + [1] * (max_num_size - d))\n",
    "num_mask = torch.ByteTensor(num_mask)\n",
    "print(num_mask)\n",
    "#num_mask的作用是这样的，由于我们是batch个问题一起放进去，假设第一个问题出现了5个数字，第二个问题出现了7个数字\n",
    "#那么我们需要把第一个问题中出现的数字次数pad到7次，那么当生成第一个问题的表达式的时候，在预测每一步对应的数字时\n",
    "#需要num_mask告知模型最后两个数字对于第一个问题来说是没有的，告知的方式就是在对应的位置上加上-1e12，那么做softmax后\n",
    "#这个位置的权重就趋向于0（例外需要考虑到常数）\n",
    "\n",
    "num_pos_pad = []\n",
    "max_num_pos_size = max(num_size_batch)\n",
    "for i in range(len(num_pos_batch)):\n",
    "    temp = num_pos_batch[i] + [-1] * (max_num_pos_size-len(num_pos_batch[i]))\n",
    "    num_pos_pad.append(temp)\n",
    "num_pos_pad = torch.LongTensor(num_pos_pad)\n",
    "#num_pos记录的是每一个问题中数字的位置，由于是batch个问题，需要按照batch里面出现最多数字次数来pad\n",
    "print(num_pos_pad)\n",
    "\n",
    "num_order_pad = []\n",
    "max_num_order_size = max(num_size_batch)\n",
    "for i in range(len(num_order_batch)):\n",
    "    temp = num_order_batch[i] + [0] * (max_num_order_size-len(num_order_batch[i]))\n",
    "    num_order_pad.append(temp)\n",
    "num_order_pad = torch.LongTensor(num_order_pad)\n",
    "\n",
    "print(num_order_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_nums\n",
    "#output2_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stack1_batch = copy.deepcopy(num_stack_batch)\n",
    "num_stack2_batch = copy.deepcopy(num_stack_batch)\n",
    "\n",
    "num_start2 = output2_lang.n_words - copy_nums - 2\n",
    "#num_start2代表的是在decoder2中，数字是从哪个下标开始的(N0在output2_lang.word2index中的起始位置)\n",
    "unk1 = output1_lang.word2index[\"UNK\"]\n",
    "unk2 = output2_lang.word2index[\"UNK\"]\n",
    "\n",
    "input1_var = torch.LongTensor(input1_batch).transpose(0, 1)\n",
    "input2_var = torch.LongTensor(input2_batch).transpose(0, 1)\n",
    "target1 = torch.LongTensor(target1_batch).transpose(0, 1)\n",
    "target2 = torch.LongTensor(target2_batch).transpose(0, 1)\n",
    "#将数据的形状改为(max_len,batch_size)，注意，第二个维度是batch\n",
    "parse_graph_pad = torch.LongTensor(parse_graph_batch)\n",
    "\n",
    "hidden_size=4\n",
    "padding_hidden = torch.FloatTensor([0.0 for _ in range(hidden_size)]).unsqueeze(0)\n",
    "batch_size = len(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Parse_Graph_Module(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Parse_Graph_Module, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.node_fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.node_fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.node_out = nn.Linear(hidden_size * 2, hidden_size)\n",
    "    \n",
    "    def normalize(self, graph, symmetric=True):\n",
    "        d = graph.sum(1)\n",
    "        if symmetric:\n",
    "            D = torch.diag(torch.pow(d, -0.5))\n",
    "            return D.mm(graph).mm(D)\n",
    "        else :\n",
    "            D = torch.diag(torch.pow(d,-1))\n",
    "            return D.mm(graph)\n",
    "        \n",
    "    def forward(self, node, graph):\n",
    "        graph = graph.float()\n",
    "        batch_size = node.size(0)\n",
    "        for i in range(batch_size):\n",
    "            graph[i] = self.normalize(graph[i])\n",
    "        \n",
    "        node_info = torch.relu(self.node_fc1(torch.matmul(graph, node)))\n",
    "        node_info = torch.relu(self.node_fc2(torch.matmul(graph, node_info)))\n",
    "        \n",
    "        agg_node_info = torch.cat((node, node_info), dim=2)\n",
    "        agg_node_info = torch.relu(self.node_out(agg_node_info))\n",
    "        \n",
    "        return agg_node_info\n",
    "    \n",
    "class EncoderSeq(nn.Module):\n",
    "    def __init__(self,vocab_size,pos_size,embed_model,word_embed_size,pos_embed_size,hidden_size,\n",
    "                n_layers=2,hop_size=2,dropout=0.5):\n",
    "        '''\n",
    "        这是第一个encoder，这个encoder的流程是：\n",
    "            1. 将单词嵌入成一个向量，将这个单词的词性也嵌入成一个向量，\n",
    "            2. 拼接这两个向量表示这个单词，然后送进GRU中\n",
    "            3. GRU的输出代表的是整个问题文本的语义信息，也就是论文中的H\n",
    "            4. 将H作为第一个GCN的初始节点的嵌入向量表示，也就是P0\n",
    "            5. 根据P0以及提取得到的依存句法关系矩阵，根据公式1和2计算P1，由于hop_size是2，所以还要计算P2\n",
    "            6. 输出P2\n",
    "        '''\n",
    "        super(EncoderSeq,self).__init__()\n",
    "        self.vocab_size=vocab_size#词汇空间大小\n",
    "        self.pos_size=pos_size#词性标注的词汇空间大小\n",
    "        self.word_embed_size=word_embed_size#词嵌入的维度\n",
    "        self.pos_embed_size=pos_embed_size#词性标注对应的嵌入维度\n",
    "        self.hidden_size=hidden_size\n",
    "        self.n_layers=n_layers\n",
    "        self.hop_size=hop_size\n",
    "        self.dropout_rate=dropout#每进行一次min-batch训练时，以0.5的概率随机舍弃神经元，\n",
    "        #训练时所有的神经元权重要乘以(1-0.5)\n",
    "        \n",
    "        self.pretrained_word_embedding_model=embed_model\n",
    "        self.pos_embedding_model=nn.Embedding(pos_size,embedding_dim=pos_embed_size,padding_idx=0)\n",
    "        #其中padding_idx是指，索引位置是0的单词对应的embedding是全0向量\n",
    "        self.dropout_operation=nn.Dropout(self.dropout_rate)\n",
    "        self.gru=nn.GRU(input_size=word_embed_size+pos_embed_size,hidden_size=hidden_size,\n",
    "                              num_layers=self.n_layers,dropout=self.dropout_rate,bidirectional=True)\n",
    "        #其中的dropout操作会在两层BiGRU之间加一层dropout\n",
    "        self.parse_gnn=clones(Parse_Graph_Module(hidden_size), hop_size)\n",
    "    \n",
    "    def forward(self,word_ids,pos_ids,input_length,parse_graph,hidden=None):\n",
    "        word_embedding=self.pretrained_word_embedding_model(word_ids)\n",
    "        pos_embedding=self.pos_embedding_model(pos_ids)\n",
    "        embeddings=torch.cat([word_embedding,pos_embedding],dim=2)\n",
    "        #size()==(seq_len,batch_size,word_embed_size+pos_embed_size)\n",
    "        embeddings=self.dropout_operation(embeddings)\n",
    "        \n",
    "        #pack_padded_sequence方法，看名字的意思就是将pad过的句子打包，该方法的原理是首先将pad的句子排序，句子长度在前面\n",
    "        #我们在之前已经将batch个句子按照长度排序了；然后以seq_len为第一个维度，batch为第二个维度\n",
    "        #该方法会统计每一个时间步有多少个batch，这个方法的输出是一个张量，而且已经去掉了pad位置\n",
    "        #例如，输入的张量形状是(10,6,100)其中6代表6个问题文本，10代表这6个问题中最长的有10个单词，100代表每一个单词用100维向量表示\n",
    "        #假如后两个问题文本的长度分别是7,6，那么pack_padded_sequence返回的张量长度是60-4-3,也就是去掉了所有的pad位置\n",
    "        packed=torch.nn.utils.rnn.pack_padded_sequence(embeddings,input_length)#根据input_length判断哪些位置是pad的，然后去掉\n",
    "        init_hidden=hidden\n",
    "        #packed已经包含有batch的信息\n",
    "        gru_outputs,gru_hidden=self.gru(packed,init_hidden)\n",
    "        gru_outputs,outputs_length=torch.nn.utils.rnn.pad_packed_sequence(gru_outputs)\n",
    "        #pad_packed_sequence显然是根据packedpad回原来的样子，之所以能够pad回原来的样子是因为gru_outputs中含有batch_size的信息\n",
    "        #gru_outputs.size()==(seq_len,batch_size,hidden_size*2)\n",
    "        #整个GRU的输出是前向的嵌入+反向的嵌入，而不是拼接的结果\n",
    "        gru_outputs=gru_outputs[:,:,:self.hidden_size]+gru_outputs[:,:,self.hidden_size:]\n",
    "        gru_outputs=gru_outputs.transpose(0,1)#(batch_size,seq_len,hidden_size)\n",
    "        for i in range(self.hop_size):\n",
    "            gru_outputs = self.parse_gnn[i](gru_outputs, parse_graph[:,2])#聚集邻居节点的信息\n",
    "        #相当于利用句子的依存句法信息加强了句子的嵌入式表示\n",
    "        gru_outputs=gru_outputs.transpose(0,1)#(seq_len,batch_size,hidden_size)\n",
    "        return gru_outputs,gru_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.weight.size()\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers=2\n",
    "hop_size=2\n",
    "embedding_size=5\n",
    "encoder = EncoderSeq(vocab_size=input1_lang.n_words, pos_size=input2_lang.n_words, \n",
    "                     embed_model=embed_model, word_embed_size=embedding_size, pos_embed_size=2, \n",
    "                     hidden_size=hidden_size, n_layers=n_layers, hop_size=hop_size)\n",
    "encoder_outputs, encoder_hidden = encoder(input1_var, input2_var, input_length, parse_graph_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 3, 4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()#(max_seq_len,batch_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(input_length[0])\n",
    "print(batch_size)\n",
    "print(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 18, 27, 35], [12, 18], [3, 9, 18]]\n",
      "num_pos_batch记录的是当前这batch个问题中，每一个问题中数字的位置\n",
      "4\n",
      "num_size代表的是这batch个问题中，出现数字次数最多的那个问题数字出现的次数\n",
      "tensor([[[0.0000, 0.0000, 0.1856, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2183, 0.0000],\n",
      "         [0.0000, 0.0284, 0.2207, 0.0000],\n",
      "         [0.0000, 0.0201, 0.2097, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.1904, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2035, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.2178, 0.0000],\n",
      "         [0.0000, 0.0326, 0.2214, 0.0000],\n",
      "         [0.0000, 0.0823, 0.2264, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<AsStridedBackward>)\n",
      "num_encoder_outputs代表的是每一个数字对应的embedding\n"
     ]
    }
   ],
   "source": [
    "def get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, hidden_size):\n",
    "    indices = list()\n",
    "    sen_len = encoder_outputs.size(0)\n",
    "    masked_index = []\n",
    "    temp_1 = [1 for _ in range(hidden_size)]\n",
    "    temp_0 = [0 for _ in range(hidden_size)]\n",
    "    for b in range(batch_size):\n",
    "        for i in num_pos[b]:\n",
    "            indices.append(i + b * sen_len)\n",
    "            masked_index.append(temp_0)\n",
    "        indices += [0 for _ in range(len(num_pos[b]), num_size)]\n",
    "        masked_index += [temp_1 for _ in range(len(num_pos[b]), num_size)]\n",
    "    indices = torch.LongTensor(indices)\n",
    "    masked_index = torch.ByteTensor(masked_index)\n",
    "    masked_index = masked_index.view(batch_size, num_size, hidden_size)\n",
    "    all_outputs = encoder_outputs.transpose(0, 1).contiguous()\n",
    "    all_embedding = all_outputs.view(-1, encoder_outputs.size(2))  # S x B x H -> (B x S) x H\n",
    "    all_num = all_embedding.index_select(0, indices)\n",
    "    all_num = all_num.view(batch_size, num_size, hidden_size)\n",
    "    return all_num.masked_fill_(masked_index.bool(), 0.0), masked_index\n",
    "\n",
    "\n",
    "print(num_pos_batch)\n",
    "print('num_pos_batch记录的是当前这batch个问题中，每一个问题中数字的位置')\n",
    "copy_num_len = [len(_) for _ in num_pos_batch]\n",
    "num_size = max(copy_num_len)\n",
    "print(num_size)\n",
    "print('num_size代表的是这batch个问题中，出现数字次数最多的那个问题数字出现的次数')\n",
    "num_encoder_outputs, masked_index = get_all_number_encoder_outputs(encoder_outputs, num_pos_batch, \n",
    "                                                                   batch_size, num_size, encoder.hidden_size)\n",
    "\n",
    "print(num_encoder_outputs)\n",
    "print('num_encoder_outputs代表的是每一个数字对应的embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_masked_values(tensor, mask, replace_with):\n",
    "    return tensor.masked_fill((1 - mask).bool(), replace_with)\n",
    "\n",
    "class NumEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, hop_size=2):\n",
    "        super(NumEncoder, self).__init__()\n",
    "        \n",
    "        self.node_dim = node_dim\n",
    "        self.hop_size = hop_size\n",
    "        self.num_gnn = clones(Num_Graph_Module(node_dim), hop_size)\n",
    "    \n",
    "    def forward(self, encoder_outputs, num_encoder_outputs, num_pos_pad, num_order_pad):\n",
    "        num_embedding = num_encoder_outputs.clone()\n",
    "        batch_size = num_embedding.size(0)\n",
    "        num_mask = (num_pos_pad > -1).long()\n",
    "        node_mask = (num_order_pad > 0).long()\n",
    "        greater_graph_mask = num_order_pad.unsqueeze(-1).expand(batch_size, -1, num_order_pad.size(-1)) > \\\n",
    "                        num_order_pad.unsqueeze(1).expand(batch_size, num_order_pad.size(-1), -1)\n",
    "        lower_graph_mask = num_order_pad.unsqueeze(-1).expand(batch_size, -1, num_order_pad.size(-1)) <= \\\n",
    "                        num_order_pad.unsqueeze(1).expand(batch_size, num_order_pad.size(-1), -1)\n",
    "        greater_graph_mask = greater_graph_mask.long()\n",
    "        lower_graph_mask = lower_graph_mask.long()\n",
    "        \n",
    "        diagmat = torch.diagflat(torch.ones(num_embedding.size(1), dtype=torch.long, device=num_embedding.device))\n",
    "        diagmat = diagmat.unsqueeze(0).expand(num_embedding.size(0), -1, -1)\n",
    "        graph_ = node_mask.unsqueeze(1) * node_mask.unsqueeze(-1) * (1-diagmat)\n",
    "        graph_greater = graph_ * greater_graph_mask + diagmat\n",
    "        graph_lower = graph_ * lower_graph_mask + diagmat\n",
    "        \n",
    "        for i in range(self.hop_size):\n",
    "            num_embedding = self.num_gnn[i](num_embedding, graph_greater, graph_lower)\n",
    "        \n",
    "#        gnn_info_vec = torch.zeros((batch_size, 1, encoder_outputs.size(-1)),\n",
    "#                                   dtype=torch.float, device=num_embedding.device)\n",
    "#        gnn_info_vec = torch.cat((encoder_outputs.transpose(0, 1), gnn_info_vec), dim=1)\n",
    "        gnn_info_vec = torch.zeros((batch_size, encoder_outputs.size(0)+1, encoder_outputs.size(-1)),\n",
    "                                   dtype=torch.float, device=num_embedding.device)\n",
    "        clamped_number_indices = replace_masked_values(num_pos_pad, num_mask, gnn_info_vec.size(1)-1)\n",
    "        gnn_info_vec.scatter_(1, clamped_number_indices.unsqueeze(-1).expand(-1, -1, num_embedding.size(-1)), num_embedding)\n",
    "        gnn_info_vec = gnn_info_vec[:, :-1, :]\n",
    "        gnn_info_vec = gnn_info_vec.transpose(0, 1)\n",
    "        gnn_info_vec = encoder_outputs + gnn_info_vec\n",
    "        num_embedding = num_encoder_outputs + num_embedding\n",
    "        problem_output = torch.max(gnn_info_vec, 0).values\n",
    "        \n",
    "        return gnn_info_vec, num_embedding, problem_output\n",
    "\n",
    "\n",
    "class Num_Graph_Module(nn.Module):\n",
    "    def __init__(self, node_dim):\n",
    "        super(Num_Graph_Module, self).__init__()\n",
    "        \n",
    "        self.node_dim = node_dim\n",
    "        self.node1_fc1 = nn.Linear(node_dim, node_dim)\n",
    "        self.node1_fc2 = nn.Linear(node_dim, node_dim)\n",
    "        self.node2_fc1 = nn.Linear(node_dim, node_dim)\n",
    "        self.node2_fc2 = nn.Linear(node_dim, node_dim)\n",
    "        self.graph_weight = nn.Linear(node_dim * 4, node_dim)\n",
    "        self.node_out = nn.Linear(node_dim * 2, node_dim)\n",
    "    \n",
    "    def normalize(self, graph, symmetric=True):\n",
    "        d = graph.sum(1)\n",
    "        if symmetric:\n",
    "            D = torch.diag(torch.pow(d, -0.5))\n",
    "            return D.mm(graph).mm(D)\n",
    "        else :\n",
    "            D = torch.diag(torch.pow(d,-1))\n",
    "            return D.mm(graph)\n",
    "\n",
    "    def forward(self, node, graph1, graph2):\n",
    "        graph1 = graph1.float()\n",
    "        graph2 = graph2.float()\n",
    "        batch_size = node.size(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            graph1[i] = self.normalize(graph1[i], False)\n",
    "            graph2[i] = self.normalize(graph2[i], False)\n",
    "        \n",
    "        node_info1 = torch.relu(self.node1_fc1(torch.matmul(graph1, node)))\n",
    "        node_info1 = torch.relu(self.node1_fc2(torch.matmul(graph1, node_info1)))\n",
    "        node_info2 = torch.relu(self.node2_fc1(torch.matmul(graph2, node)))\n",
    "        node_info2 = torch.relu(self.node2_fc2(torch.matmul(graph2, node_info2)))\n",
    "        gate = torch.cat((node_info1, node_info2, node_info1+node_info2, node_info1-node_info2), dim=2)\n",
    "        gate = torch.sigmoid(self.graph_weight(gate))\n",
    "        node_info = gate * node_info1 + (1-gate) * node_info2\n",
    "        agg_node_info = torch.cat((node, node_info), dim=2)\n",
    "        agg_node_info = torch.relu(self.node_out(agg_node_info))\n",
    "        \n",
    "        return agg_node_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "numencoder = NumEncoder(node_dim=hidden_size, hop_size=hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, num_outputs, problem_output = numencoder(encoder_outputs, num_encoder_outputs, \n",
    "                                                          num_pos_pad, num_order_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3340, -0.2522,  0.0755, -0.2489],\n",
       "        [ 0.2480,  0.9542,  0.5509, -0.4507],\n",
       "        [-0.2658,  0.7542, -0.2905,  1.0498]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden[1]+encoder_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input=torch.LongTensor([output2_lang.word2index['SOS'] for _ in range(batch_size)])#第一个时间步都是SOS\n",
    "max_target_length=max(target2_length)#当前这个批次最长的生成长度不能超过最长的表达式\n",
    "#现在假设使用教师指导的方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2275, -0.1594,  0.1194,  0.3921],\n",
       "         [ 0.0883,  0.4307, -0.1044,  0.2233],\n",
       "         [-0.3471,  0.3757,  0.1453,  0.5677]],\n",
       "\n",
       "        [[-0.5615, -0.0928, -0.0439, -0.6410],\n",
       "         [ 0.1597,  0.5235,  0.6553, -0.6740],\n",
       "         [ 0.0813,  0.3786, -0.4358,  0.4821]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden=encoder_hidden[:2]\n",
    "decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "[[ 2.91910714  2.97226749  1.08492904 -1.23719192]]\n",
      "[[ 2.91910714  2.97226749  1.08492904 -1.23719192]]\n"
     ]
    }
   ],
   "source": [
    "W_a=np.random.randn(3,4)\n",
    "W_b=np.random.randn(2,4)\n",
    "W_c=np.concatenate((W_a,W_b),0)\n",
    "print(W_c.shape)\n",
    "a=np.random.randn(1,3)\n",
    "b=np.random.randn(1,2)\n",
    "print(np.dot(a,W_a)+np.dot(b,W_b))\n",
    "print(np.dot(np.concatenate((a,b),1),W_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        '''\n",
    "        W_a*a+W_b*b==W_ab*[a;b]\n",
    "        '''\n",
    "        super(Attn,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.attn=nn.Linear(in_features=hidden_size*2,out_features=hidden_size)\n",
    "        self.score=nn.Linear(in_features=hidden_size,out_features=1,bias=False)\n",
    "        #score的目的是将向量映射为一个数值表示这个单词的重要程度\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs,seq_mask=None):\n",
    "        '''\n",
    "        hidden.size()==(batch_size,hidden_size)\n",
    "        encoder_outputs.size()==(batch_size,seq_len,hidden_size)\n",
    "        做attention的流程是att_weights=softmax(torch.bmm(hidden.unsqueeze(1),encoder_outputs.transpose(1,2))\n",
    "        以上是最常规的做法，利用向量内积的方式提取出hidden和encoder_outputs之间的相关性\n",
    "        而在这篇源码中，利用的是神经网络的方式，也就是\n",
    "        att_weights=softmax(W_s*tanh((W_h*hidden+b_h)+(W_e*encoder_outputs+b_e)))\n",
    "        '''\n",
    "        \n",
    "        seq_len,batch_size,hidden_size=encoder_outputs.size()\n",
    "        #assert hidden.dim()==3\n",
    "        repeat_dims=[1]*hidden.dim()\n",
    "        repeat_dims[0]=seq_len#广播操作\n",
    "        hidden=hidden.repeat(*repeat_dims)#(seq_len,batch_size,hidden_size)\n",
    "        \n",
    "        energy_in=torch.cat((hidden,encoder_outputs),2).view(-1,2*self.hidden_size)\n",
    "        att_energies=self.score(torch.tanh(self.attn(energy_in)))#(batch_size*seq_len,1)\n",
    "        att_energies=att_energies.squeeze(1)#(batch_size*seq_len)\n",
    "        att_energies=att_energies.view(seq_len,batch_size).transpose(0,1)\n",
    "        if seq_mask is not None:\n",
    "            att_energies=att_energies.masked_fill_(seq_mask.bool(),-1e12)\n",
    "        att_energies=self.softmax(att_energies)\n",
    "        return att_energies.unsqueeze(1)#(batch_size,1,seq_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding_size,input_size,output_size,n_layers=2,dropout_rate=0.5):\n",
    "        '''\n",
    "        hidden_size就是decoder端的RNN的hidden_size\n",
    "        embedding_size是decoder端嵌入层的维度，也是RNN输入的维度\n",
    "        input_size和output_size就是output2_lang中有少个字符，一共有25个，分别是+-/*以及Ni，还有1和3.14以及SOS,EOS等\n",
    "        '''\n",
    "        super(AttnDecoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.n_layers=n_layers\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.dropout_rate=dropout_rate\n",
    "        \n",
    "        self.dropout_operation=nn.Dropout(self.dropout_rate)#dropout操作可以缓解过拟合问题，因为每一次网络结构会变小\n",
    "        #最后在测试阶段，所有的权重都要乘以（1-dropout_rate），有集成学习的思想\n",
    "        self.embedding_model=nn.Embedding(input_size,embedding_size,padding_idx=0)\n",
    "        self.gru=nn.GRU(hidden_size+embedding_size,hidden_size,n_layers,dropout=self.dropout_rate)\n",
    "        #GRU与LSTM的区别在于，GRU的更新门同时作为遗忘门和输入门的功能\n",
    "        #GRU的输入是context向量与input token的embedding的拼接，会在两层之间做一次dropout操作,如果n_layers=1，那么dropout操作是不会做的\n",
    "        self.concat_affine_layer=nn.Linear(in_features=hidden_size*2,out_features=hidden_size)\n",
    "        #注意，预测当前时间步的token时，不仅仅考虑RNN的输出，还要考虑context这个上下文向量，因此需要将当前时间步的\n",
    "        #GRU的hidden_state和context拼接\n",
    "        self.output_layer=nn.Linear(in_features=hidden_size,out_features=self.output_size)#将向量映射成输出词汇空间上的概率分布\n",
    "        self.att_layer=Attn(hidden_size)#上一时间步的hidden_state会和encoder_outputs做注意力，得到context\n",
    "        \n",
    "    \n",
    "    def forward(self,decoder_input,last_hidden,encoder_outputs,seq_mask):\n",
    "        '''\n",
    "        decoder就是根据当前时间步的decoder_input(size()==(batch_size,))，嵌入成embeddings\n",
    "        last_hidden就是上一时刻的隐藏状态h_{t-1}\n",
    "        典型的seq2seq结构在解码时，要拿上一时刻的隐藏状态，也就是h_{t-1}和encoder_outputs做注意力运算\n",
    "        目的是提取出encoder端在当前时间步有用的信息，提取出来的张量叫context，然后将context和当前时间步的嵌入embeddings\n",
    "        拼接作为当前时间步的输入x_{t}，再加上last_hidden，然后得到当前时间步的隐藏状态current_hidden\n",
    "        以上就是典型的seq2seq结构在解码端的运作机制\n",
    "        '''\n",
    "        batch_size=decoder_input.size(0)#decoder_input.size()==(batch_size,)\n",
    "        current_embeddings=self.dropout_operation(self.embedding_model(decoder_input))#(batch_size,embed_dim)\n",
    "        current_embeddings=current_embeddings.unsqueeze(0)#(1,batch_size,embed_dim)\n",
    "        #last_hidden.size()==(n_layers,batch_size,hidden_size),n_layers表示有几层RNN\n",
    "        att_weights=self.att_layer(last_hidden[-1].unsqueeze(0),encoder_outputs,seq_mask)#此时每一个数字代表对应的token的重要程度\n",
    "        context=torch.bmm(att_weights,encoder_outputs.transpose(0,1))#context是encoder-aware representation\n",
    "        #context.size()==(batch_size,1,hidden_size)\n",
    "        #接下来把context连同decoder_input_embedding作为当前时间步GRU的输入\n",
    "        rnn_output,hidden=self.gru(torch.cat((current_embeddings,context.transpose(0,1)),2),last_hidden)\n",
    "        #rnn_output.size()==(1,batch_size,hidden_size)\n",
    "        #接下来就是利用rnn_output作为预测的依据来生成表达式的单词\n",
    "        concat_vector=torch.cat((rnn_output.squeeze(0),context.squeeze(1)),1)#(batch_size,hidden_size*2)\n",
    "        output=self.output_layer(torch.tanh(self.concat_affine_layer(concat_vector)))#(batch_size,output_size)\n",
    "        return output,hidden\n",
    "    \n",
    "decoder=AttnDecoderRNN(hidden_size=hidden_size,embedding_size=10,input_size=output2_lang.n_words,\n",
    "                       output_size=output2_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder端的词汇空间 :  {'-': 0, '*': 1, '+': 2, '/': 3, '^': 4, 'PAD': 5, 'EOS': 6, '1': 7, '3.14': 8, 'N0': 9, 'N1': 10, 'N2': 11, 'N3': 12, 'N4': 13, 'N5': 14, 'N6': 15, 'N7': 16, 'N8': 17, 'N9': 18, 'N10': 19, 'N11': 20, 'N12': 21, 'N13': 22, 'N14': 23, 'SOS': 24, 'UNK': 25} \n",
      " 一共有26个词汇\n",
      "初始时刻的decoder_input是batch个SOS token :  tensor([24, 24, 24])\n",
      "decoder_hidden代表的是上一个时间步的隐藏状态，初始时刻利用encoder端GRU的最后一个时间步的hidden_state作为decoder端的地一个时间步的hidden_state\n",
      "torch.Size([2, 3, 4]) 其中2表示2层GRU，我们实际只用最后一层的hidden state;3代表batch个数,4表示hidden_size\n",
      "torch.Size([46, 3, 4]) 其中第一个维度代表的是这batch个句子的最长的句子的序列长度，如果某个句子不够长，那就pad 0\n",
      "torch.Size([3, 46]) seq_mask用来指明那些位置的单词是pad的，做注意力的时候需要seq_mask\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder端的词汇空间 : \",output2_lang.word2index,\"\\n\",\"一共有%d个词汇\"%output2_lang.n_words)\n",
    "print(\"初始时刻的decoder_input是batch个SOS token : \",decoder_input)\n",
    "print(\"decoder_hidden代表的是上一个时间步的隐藏状态，初始时刻利用encoder端GRU的最后一个时间步的hidden_state作为decoder端的地一个时间步的hidden_state\")\n",
    "print(decoder_hidden.size(),\"其中2表示2层GRU，我们实际只用最后一层的hidden state;3代表batch个数,4表示hidden_size\")\n",
    "print(encoder_outputs.size(),\"其中第一个维度代表的是这batch个句子的最长的句子的序列长度，如果某个句子不够长，那就pad 0\")\n",
    "print(seq_mask.size(),\"seq_mask用来指明那些位置的单词是pad的，做注意力的时候需要seq_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output,decoder_hidden=decoder(decoder_input,last_hidden=decoder_hidden,encoder_outputs=encoder_outputs,seq_mask=seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 26])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 10,  9],\n",
       "        [12,  9, 10],\n",
       "        [ 0,  7,  0],\n",
       "        [10,  9, 11],\n",
       "        [11,  0,  0],\n",
       "        [ 1,  0, 10],\n",
       "        [10,  3,  0],\n",
       "        [ 0,  6,  6],\n",
       "        [ 3,  0,  0],\n",
       "        [ 6,  0,  0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 26])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, '*': 1, '+': 2, '/': 3, '^': 4, 'PAD': 5, 'EOS': 6, '1': 7, '3.14': 8, 'N0': 9, 'N1': 10, 'N2': 11, 'N3': 12, 'N4': 13, 'N5': 14, 'N6': 15, 'N7': 16, 'N8': 17, 'N9': 18, 'N10': 19, 'N11': 20, 'N12': 21, 'N13': 22, 'N14': 23, 'SOS': 24, 'UNK': 25}\n"
     ]
    }
   ],
   "source": [
    "print(output2_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoder_input(target,decoder_output,num_stack_batch,num_start,unk):\n",
    "    '''\n",
    "    如果你的数据集中不含有重复数字，那么不需要这个函数\n",
    "    target的长度是batch_size,每一个值代表一个样本在当前时间步的标签\n",
    "    decoder_output的长度是batch_size,每一个值是长度为26的向量，26是num_classes\n",
    "    num_start是指在decoder的词汇空间中，数字是从哪个下标开始的，比如\n",
    "    {'-': 0, '*': 1, '+': 2, '/': 3, '^': 4, 'PAD': 5, 'EOS': 6, '1': 7, '3.14': 8, 'N0': 9, 'N1': 10, 'N2': 11, 'N3': 12, 'N4': 13, 'N5': 14, 'N6': 15, 'N7': 16, 'N8': 17, 'N9': 18, 'N10': 19, 'N11': 20, 'N12': 21, 'N13': 22, 'N14': 23, 'SOS': 24, 'UNK': 25}\n",
    "    那么num_start是9\n",
    "    \n",
    "    例：\n",
    "    问题文本：小芳家5月份用水量是16.5吨，每吨水的价格是2.1元，小芳家一共有5口人，平均每人应交多少水费？\n",
    "    转换后的输入文本：['小', '芳', '家', 'NUM', '月份', '用水量', '是', 'NUM', '吨', '，', '每吨', '水', '的', '价格', '是', 'NUM', '元', '，', '小', '芳', '家', '一共', '有', 'NUM', '口', '人', '，', '平均', '每人', '应交', '多少', '水费', '？']\n",
    "    前缀表达式:  ['/', '*', 'N1', 'N2', '5']\n",
    "    '''\n",
    "    \n",
    "    for i in range(target.size(0)):\n",
    "        if target[i]==unk:\n",
    "            #说明这个位置是重复数字\n",
    "            assert num_stack_batch!=[]\n",
    "            num_stack=num_stack_batch[i].pop()\n",
    "            assert num_stack!=[]#以上面的例子，那么num_stack就是[[0,3]]代表有一个重复数字出现，位置分别是出现在\n",
    "            #第一个和第四个数字\n",
    "            max_score=-float('100000')#选取decoder_outputs在这两个位置中分数大的那个位置\n",
    "            #因为不管怎么样，这两个位置的数字的数值是一样的\n",
    "            for num_pos in num_stack:\n",
    "                #num_pos要么是0，要么是3\n",
    "                if decoder_output[i,num_start+num]>max_score:\n",
    "                    #num_start+num正好对应着N0和N3，这也正是上面那个例子中问题文本中重复数字出现的位置顺序\n",
    "                    max_score=decoder_output[i,num_start+num]\n",
    "                    target[i]=num_start+num\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decoder_outputs=[]\n",
    "num_start=9\n",
    "unk=25\n",
    "decoder_input=generate_decoder_input(target2[0],decoder_output,num_stack2_batch,num_start,unk)\n",
    "decoder_output,decoder_hidden=decoder(decoder_input,\n",
    "                                      last_hidden=decoder_hidden,\n",
    "                                      encoder_outputs=encoder_outputs,seq_mask=seq_mask)\n",
    "all_decoder_outputs.append(decoder_output)\n",
    "\n",
    "decoder_input=generate_decoder_input(target2[1],decoder_output,num_stack2_batch,num_start,unk)\n",
    "decoder_output,decoder_hidden=decoder(decoder_input,\n",
    "                                      last_hidden=decoder_hidden,\n",
    "                                      encoder_outputs=encoder_outputs,seq_mask=seq_mask)\n",
    "all_decoder_outputs.append(decoder_output)\n",
    "\n",
    "decoder_input=generate_decoder_input(target2[2],decoder_output,num_stack2_batch,num_start,unk)\n",
    "decoder_output,decoder_hidden=decoder(decoder_input,\n",
    "                                      last_hidden=decoder_hidden,\n",
    "                                      encoder_outputs=encoder_outputs,seq_mask=seq_mask)\n",
    "all_decoder_outputs.append(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951942711828836"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 12, 0, 10, 11, 1, 10, 0, 3, 6], [10, 9, 7, 9, 0, 0, 3, 6, 0, 0], [9, 10, 0, 11, 0, 10, 0, 6, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(target2_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 10,  9],\n",
       "        [12,  9, 10],\n",
       "        [ 0,  7,  0],\n",
       "        [10,  9, 11],\n",
       "        [11,  0,  0],\n",
       "        [ 1,  0, 10],\n",
       "        [10,  3,  0],\n",
       "        [ 0,  6,  6],\n",
       "        [ 3,  0,  0],\n",
       "        [ 6,  0,  0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 8]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss=-$\\sum_{i=1}^{N}\\sum_{j=1}^{T}y_j^{(i)}\\log \\hat{y}_{j}^{(i)}$\n",
    "其中N是指batch,T是指序列长度，$y_j^{(i)}$代表第$i$个样本在第$j$个时间步的标签，\n",
    "$\\hat{y}_{j}^{(i)}$代表第$i$个样本在第$j$个时间步预测的token分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length = max(target2_length)\n",
    "all_decoder_outputs = torch.zeros(max_target_length, batch_size, decoder.output_size)\n",
    "for t in range(max_target_length):\n",
    "    decoder_output, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs, seq_mask)\n",
    "    all_decoder_outputs[t] = decoder_output#use_teacher_forcing初始值是0.83，因为前期还是需要教师指导的\n",
    "    decoder_input = generate_decoder_input(\n",
    "        target2[t], decoder_output, num_stack_batch, num_start, unk)\n",
    "    target2[t] = decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 26])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_decoder_outputs.size()#(max_target_len,batch_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "all_decoder_outputs=all_decoder_outputs.transpose(0,1).contiguous()#(batch_size,max_target_len,num_classes)\n",
    "target=target2.transpose(0,1)#(batch_size,max_target_len)\n",
    "\n",
    "logits_flat=all_decoder_outputs.view(-1,all_decoder_outputs.size(-1))#(batch_size*max_target_len,num_classes)\n",
    "log_probs_flat=torch.nn.functional.log_softmax(logits_flat)#(batch_size*max_target_len,num_classes)\n",
    "target_flat=target.view(-1,1)#(batch_size*max_target_len,1)\n",
    "loss_flat=-torch.gather(log_probs_flat,dim=1,index=target_flat)#(batch_size*max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_flat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4804, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(loss_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4804, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.nll_loss(torch.log_softmax(logits_flat,dim=1),target_flat.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4804, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss()(logits_flat,target_flat.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
      "        [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
      "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "target_length=[9,7,10]\n",
    "max_target_len=max(target_length)\n",
    "seq_range=torch.arange(0,max_target_len).long().unsqueeze(0).expand(len(target_length),max_target_len)\n",
    "print(seq_range)\n",
    "target_length=torch.LongTensor(target_length)\n",
    "target_length=target_length.unsqueeze(1).expand_as(seq_range)\n",
    "print(target_length)\n",
    "target_mask=(seq_range<target_length).float()\n",
    "print(target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
